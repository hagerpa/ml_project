{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, re, pickle, itertools, progressbar, importlib\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from categories import categories\n",
    "import filters, vocabulary_tester, vocabulary_builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cats = categories()\n",
    "\n",
    "qcatfile = open('question_category_train.csv', 'r')\n",
    "qcatreader = csv.reader(qcatfile)\n",
    "\n",
    "next(qcatreader) # skipping column discription\n",
    "\n",
    "qcat_dict = {} # mapping from question_id to the parent category_id\n",
    "cat_freq = nltk.FreqDist() # maps the category_id to its\n",
    "\n",
    "for qcat in qcatreader:\n",
    "    cat_id = int(qcat[1])\n",
    "    pcat_id = cats.parent_id(cat_id)\n",
    "    q_id = int(qcat[2])\n",
    "    \n",
    "    qcat_dict[q_id] = pcat_id\n",
    "    cat_freq[cats.name(cat_id)] += 1\n",
    "    \n",
    "#cat_freq.plot()\n",
    "#cat_freq.most_common(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(filters)\n",
    "\n",
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "next(qreader)\n",
    "\n",
    "questions = []\n",
    "vocabulary = {}\n",
    "vocabulary['all'] = nltk.FreqDist()\n",
    "for cat_name in cats.all_names(): vocabulary[cat_name] = nltk.FreqDist()\n",
    "\n",
    "# Set this parameter to TRUE if you want to read through\n",
    "# all questions again, elsewise from file set to FALSE.\n",
    "NewRead = False \n",
    "\n",
    "total_enteties_count = 0\n",
    "\n",
    "if NewRead:\n",
    "    bar = progressbar.ProgressBar()\n",
    "    \n",
    "    for row in bar(qreader):\n",
    "        if len(row) != 21: continue\n",
    "        if row[15] != \"0\": continue\n",
    "            \n",
    "        if int(row[0]) in qcat_dict.keys():\n",
    "            cat_id = qcat_dict[int(row[0])]\n",
    "            \n",
    "            sentence = row[4].lower()\n",
    "            # running a sequence of filters on the raw question string \n",
    "            for filt in [filters.punctuation_filter]:\n",
    "                sentence = filt(sentence)\n",
    "            \n",
    "            words = word_tokenize(sentence)\n",
    "            # running a sequence of filtes on the already tokenized sentence\n",
    "            for filt in [filters.year_tracker, filters.small_word_filter, filters.stemming_filter]:\n",
    "                words = filt(words)\n",
    "            \n",
    "            questions.append( {\"words\":words, \"cat\":cats.name(cat_id)} )\n",
    "            vocabulary[ cats.name(cat_id) ] += nltk.FreqDist(words)\n",
    "            vocabulary['all'] += nltk.FreqDist(words)\n",
    "        \n",
    "    ## Saving into pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'wb'), open('vocabulary.pkl', 'wb')\n",
    "    pickle.dump(questions, q_file)\n",
    "    pickle.dump(vocabulary, v_file)\n",
    "    \n",
    "else:\n",
    "    ## Loading from pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'rb'), open('vocabulary.pkl', 'rb')\n",
    "    questions, vocabulary = pickle.load(q_file), pickle.load(v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(vocabulary_tester)\n",
    "importlib.reload(vocabulary_builders)\n",
    "from vocabulary_builders import most_common, most_common_reduced, ig_based\n",
    "\n",
    "corpus = [(q['words'], q['category']) for q in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing IG-based Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_builders = {}\n",
    "for M in np.arange(50,501,50):\n",
    "    name = \"ig_based (M = {0})\".format(M)\n",
    "    vocab_builders[ name ] = (ig_based, {\"frequencies\": vocabulary,\n",
    "                                         \"cat_frequencies\": cat_freq,\n",
    "                                         \"categories\": cats.all_names(),\n",
    "                                         \"M\": M,\n",
    "                                         \"read_from_file\": True})\n",
    "\n",
    "vocabularies = {}\n",
    "for vb_name in vocab_builders.keys():\n",
    "    vb, args = vocab_builders[vb_name]\n",
    "    vocabularies[vb_name] = vb(**args)\n",
    "\n",
    "vocabulary_tester.test(vocabularies, corpus, tr_set_size=10000, te_set_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_reduced (M = 50, S = 12, MS = 50) done with M = 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 500) done with M = 500\n",
      "most_common_reduced (M = 400, S = 10, MS = 100) done with M = 400\n",
      "most_common_reduced (M = 100, S = 8, MS = 150) done with M = 100\n",
      "most_common_reduced (M = 350, S = 9, MS = 50) done with M = 350\n",
      "most_common_reduced (M = 450, S = 9, MS = 50) done with M = 450\n",
      "most_common_reduced (M = 50, S = 11, MS = 100) done with M = 50\n",
      "most_common_reduced (M = 500, S = 12, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 450, S = 11, MS = 50) done with M = 450\n",
      "most_common_reduced (M = 50, S = 9, MS = 150) done with M = 50\n",
      "most_common_reduced (M = 500, S = 8, MS = 50) done with M = 500\n",
      "most_common_reduced (M = 100, S = 9, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 350, S = 8, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 450, S = 12, MS = 150) done with M = 450\n",
      "most_common_reduced (M = 150, S = 12, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 450, S = 11, MS = 150) done with M = 450\n",
      "most_common_reduced (M = 500, S = 11, MS = 50) done with M = 500\n",
      "most_common_reduced (M = 350, S = 11, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 200, S = 8, MS = 100) done with M = 200\n",
      "most_common (M = 500) done with M = 500\n",
      "most_common_reduced (M = 100, S = 9, MS = 50) done with M = 100\n",
      "most_common_reduced (M = 150, S = 8, MS = 100) done with M = 150\n",
      "most_common_reduced (M = 450, S = 11, MS = 100) done with M = 450\n",
      "most_common_reduced (M = 300, S = 8, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 100, S = 10, MS = 150) done with M = 100\n",
      "most_common (M = 50) done with M = 50\n",
      "most_common_reduced (M = 500, S = 10, MS = 150) done with M = 500\n",
      "most_common_reduced (M = 300, S = 10, MS = 50) done with M = 300\n",
      "most_common_reduced (M = 300, S = 10, MS = 150) done with M = 300\n",
      "most_common_reduced (M = 150, S = 9, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 350, S = 10, MS = 50) done with M = 350\n",
      "most_common_reduced (M = 200, S = 12, MS = 50) done with M = 200\n",
      "most_common_reduced (M = 100, S = 9, MS = 150) done with M = 100\n",
      "most_common_reduced (M = 300, S = 10, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 150, S = 10, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 200, S = 10, MS = 100) done with M = 200\n",
      "most_common_reduced (M = 450, S = 9, MS = 100) done with M = 450\n",
      "most_common_reduced (M = 250, S = 9, MS = 50) done with M = 250\n",
      "most_common (M = 150) done with M = 150\n",
      "most_common_reduced (M = 250, S = 10, MS = 100) done with M = 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 200) done with M = 200\n",
      "most_common_reduced (M = 350, S = 8, MS = 150) done with M = 350\n",
      "most_common_reduced (M = 350, S = 9, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 400, S = 12, MS = 100) done with M = 400\n",
      "most_common_reduced (M = 100, S = 11, MS = 50) done with M = 100\n",
      "most_common_reduced (M = 500, S = 9, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 50, S = 8, MS = 50) done with M = 50\n",
      "most_common_reduced (M = 50, S = 11, MS = 150) done with M = 50\n",
      "most_common_reduced (M = 250, S = 12, MS = 100) done with M = 250\n",
      "most_common_reduced (M = 350, S = 11, MS = 150) done with M = 350\n",
      "most_common_reduced (M = 500, S = 11, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 150, S = 10, MS = 100) done with M = 150\n",
      "most_common (M = 350) done with M = 350\n",
      "most_common_reduced (M = 500, S = 10, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 500, S = 8, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 200, S = 10, MS = 150) done with M = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 300) done with M = 300\n",
      "most_common (M = 200) done with M = 200\n",
      "most_common_reduced (M = 300, S = 9, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 400, S = 9, MS = 150) done with M = 400\n",
      "most_common_reduced (M = 200, S = 12, MS = 100) done with M = 200\n",
      "most_common_reduced (M = 50, S = 12, MS = 150) done with M = 50\n",
      "most_common_reduced (M = 350, S = 12, MS = 50) done with M = 350\n",
      "most_common_reduced (M = 100, S = 11, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 250, S = 12, MS = 150) done with M = 250\n",
      "most_common_reduced (M = 50, S = 9, MS = 50) done with M = 50\n",
      "most_common_reduced (M = 300, S = 11, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 200, S = 11, MS = 100) done with M = 200\n",
      "most_common_reduced (M = 200, S = 11, MS = 150) done with M = 200\n",
      "most_common_reduced (M = 450, S = 8, MS = 150) done with M = 450\n",
      "most_common_reduced (M = 150, S = 8, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 250, S = 11, MS = 100) done with M = 250\n",
      "most_common_reduced (M = 300, S = 12, MS = 150) done with M = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 100) done with M = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 400) done with M = 400\n",
      "most_common_reduced (M = 350, S = 11, MS = 50) done with M = 350\n",
      "most_common_reduced (M = 100, S = 8, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 150, S = 9, MS = 150) done with M = 150\n",
      "most_common_reduced (M = 300, S = 12, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 50, S = 12, MS = 100) done with M = 50\n",
      "most_common_reduced (M = 100, S = 11, MS = 150) done with M = 100\n",
      "most_common_reduced (M = 500, S = 9, MS = 150) done with M = 500\n",
      "most_common_reduced (M = 150, S = 11, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 250, S = 8, MS = 150) done with M = 250\n",
      "most_common_reduced (M = 350, S = 12, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 350, S = 10, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 150, S = 9, MS = 100) done with M = 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% ( 0 of 14) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_reduced (M = 400, S = 8, MS = 100) done with M = 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 150) done with M = 150\n",
      "most_common_reduced (M = 150, S = 11, MS = 100) done with M = 150\n",
      "most_common_reduced (M = 50, S = 8, MS = 150) done with M = 50\n",
      "most_common_reduced (M = 50, S = 9, MS = 100) done with M = 50\n",
      "most_common_reduced (M = 200, S = 9, MS = 100) done with M = 200\n",
      "most_common_reduced (M = 300, S = 8, MS = 50) done with M = 300\n",
      "most_common_reduced (M = 50, S = 8, MS = 100) done with M = 50\n",
      "most_common_reduced (M = 300, S = 9, MS = 150) done with M = 300\n",
      "most_common_reduced (M = 300, S = 8, MS = 150) done with M = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 350) done with M = 350\n",
      "most_common_reduced (M = 100, S = 10, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 400, S = 10, MS = 50) done with M = 400\n",
      "most_common_reduced (M = 250, S = 10, MS = 150) done with M = 250\n",
      "most_common (M = 400) done with M = 400\n",
      "most_common_reduced (M = 250, S = 11, MS = 50) done with M = 250\n",
      "most_common_reduced (M = 200, S = 8, MS = 50) done with M = 200\n",
      "most_common_reduced (M = 250, S = 9, MS = 150) done with M = 250\n",
      "most_common (M = 100) done with M = 100\n",
      "most_common_reduced (M = 250, S = 12, MS = 50) done with M = 250\n",
      "most_common_reduced (M = 150, S = 8, MS = 150) done with M = 150\n",
      "most_common_reduced (M = 350, S = 9, MS = 150) done with M = 350\n",
      "most_common_reduced (M = 100, S = 12, MS = 50) done with M = 100\n",
      "most_common_reduced (M = 400, S = 9, MS = 50) done with M = 400\n",
      "most_common_reduced (M = 450, S = 9, MS = 150) done with M = 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 250) done with M = 250\n",
      "most_common_reduced (M = 250, S = 8, MS = 100) done with M = 250\n",
      "most_common_reduced (M = 400, S = 11, MS = 100) done with M = 400\n",
      "most_common_reduced (M = 500, S = 9, MS = 50) done with M = 500\n",
      "most_common_reduced (M = 100, S = 12, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 100, S = 10, MS = 50) done with M = 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for M in np.arange(50,501,50):\n",
    "    name = \"ig_based (M = {0})\".format(M)\n",
    "    vocab_builders[ name ] = (ig_based, {\"frequencies\": vocabulary,\n",
    "                                         \"cat_frequencies\": cat_freq,\n",
    "                                         \"categories\": cats.all_names(),\n",
    "                                         \"M\": M,\n",
    "                                         \"read_from_file\": True})\n",
    "    \n",
    "    name = \"most_common (M = {0})\".format(M)\n",
    "    vocab_builders[ name ] = (most_common, {\"frequencies\":vocabulary,\n",
    "                                            \"categories\": cats.all_names(),\n",
    "                                            \"M\": M})\n",
    "    \n",
    "    for S in np.arange(8,13):\n",
    "        for MS in [50,100,150]:\n",
    "            name = \"most_common_reduced (M = {0}, S = {1}, MS = {2})\".format(M,S,MS)\n",
    "            vocab_builders[ name ] = (most_common_reduced, {\"frequencies\":vocabulary,\n",
    "                                                                           \"categories\": cats.all_names(),\n",
    "                                                                           \"M\": M})\n",
    "\n",
    "vocabularies = {}\n",
    "for vb_name in vocab_builders.keys():\n",
    "    vb, args = vocab_builders[vb_name]\n",
    "    vocabularies[vb_name] = vb(**args)\n",
    "    print(vb_name,\"done with M =\",args[\"M\"])\n",
    "\n",
    "corpus = [(q['words'], q['category']) for q in questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
