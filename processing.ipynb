{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "import pickle\n",
    "from categories import categories\n",
    "import filters\n",
    "import importlib\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: question_id               1: user_id                   2: sms_guru_id               3: category_main_id         \n",
      " 4: question                  5: description               6: tags                      7: categories               \n",
      " 8: url                       9: rating_count_positive    10: rating_count_negative    11: answer_count             \n",
      "12: reported                 13: answered                 14: active                   15: deleted                  \n",
      "16: seo_locked               17: editor_locked            18: editor_id                19: created_at               \n",
      "20: updated_at               "
     ]
    }
   ],
   "source": [
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "#cols = list(zip(np.arange(21),next(reader)))\n",
    "j = 0\n",
    "for i, q in zip(np.arange(21), next(qreader)):\n",
    "    if j == 3: l = \"\\n\"; j = 0;\n",
    "    else: l = \"\"; j += 1\n",
    "        \n",
    "    print('{0:2}: {1:25}'.format(i,q), end=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading out questions and tokenizing, checking vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = categories()\n",
    "\n",
    "qcatfile = open('question_category_train.csv', 'r')\n",
    "qcatreader = csv.reader(qcatfile)\n",
    "\n",
    "next(qcatreader) # skipping column discription\n",
    "\n",
    "qcat_dict = {} # mapping from question_id to the parent category_id\n",
    "catq_count = nltk.FreqDist() # maps the category_id to its\n",
    "\n",
    "for qcat in qcatreader:\n",
    "    cat_id = int(qcat[1])\n",
    "    pcat_id = cats.parent_id(cat_id)\n",
    "    q_id = int(qcat[2])\n",
    "    \n",
    "    qcat_dict[q_id] = pcat_id\n",
    "    catq_count[cats.name(cat_id)] += 1\n",
    "    \n",
    "#catq_count.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(filters)\n",
    "\n",
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "next(qreader)\n",
    "\n",
    "questions = []\n",
    "vocabulary = {}\n",
    "for cat_name in cats.all_names():\n",
    "    vocabulary[cat_name] = nltk.FreqDist()\n",
    "\n",
    "# Set this parameter to TRUE if you want to read through\n",
    "# all questions again, elsewise from file set to FALSE.\n",
    "NewRead = True \n",
    "\n",
    "if NewRead:\n",
    "    for row in qreader:\n",
    "        if len(row) == 21:\n",
    "            if int(row[0]) in qcat_dict.keys():\n",
    "                cat_id = qcat_dict[int(row[0])]\n",
    "                \n",
    "                sentence = row[4].lower()\n",
    "                # running a sequence of filters on the raw question string \n",
    "                for filt in [filters.punctuation_filter]:\n",
    "                    sentence = filt(sentence)\n",
    "                \n",
    "                words = word_tokenize(sentence)\n",
    "                # running a sequence of filtes on the already tokenized sentence\n",
    "                for filt in [filters.year_tracker, filters.small_word_filter, filters.stemming_filter]:\n",
    "                    words = filt(words)\n",
    "                \n",
    "                questions += [{\"words\": words, \"cat_id\": cat_id}]\n",
    "                vocabulary[ cats.name(cat_id) ] += nltk.FreqDist(words)\n",
    "        \n",
    "    ## Saving into pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'wb'), open('vocabulary.pkl', 'wb')\n",
    "    pickle.dump(questions, q_file)\n",
    "    pickle.dump(vocabulary, v_file)\n",
    "    \n",
    "else:\n",
    "    ## Loading from pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'rb'), open('vocabulary.pkl', 'rb')\n",
    "    questions, vocabulary = pickle.load(q_file), pickle.load(v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film_and_musik', 'stars_and_promis', 'computer_and_pc', 'alltag', 'namensforschung', 'literatur_and_sprache', 'schule', 'mensch_and_koerper', 'freizeit_and_sport', 'wissen', 'liebe_and_beziehung', 'astrologie', 'games_and_spiele', 'adult']\n",
      "[('ein', 47), ('wie', 47), ('ist', 45), ('ich', 44), ('man', 37), ('was', 36), ('die', 34), ('kann', 29), ('der', 23), ('und', 22), ('es', 22), ('auf', 22), ('wo', 19), ('bei', 19), ('in', 18), ('das', 18), ('welch', 16), ('den', 15), ('gibt', 14), ('mit', 14)]\n",
      "['erinn', 'mail', 'kernel', 'suchbegriff', 'youtub', 'doch', 'verkauft', 'handy', 'warum', 'soweit', 'frau', 'comput', '2008', 'crysis', 'absend', 'fir', '4', 'kommt', '000', '100kg']\n"
     ]
    }
   ],
   "source": [
    "print(cats.all_names())\n",
    "print(vocabulary['computer_and_pc'].most_common(20))\n",
    "print(list(vocabulary['computer_and_pc'])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_features(vocab, question):\n",
    "    features = {}\n",
    "    for v in vocab:\n",
    "        features[v] = v in question['words']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 150 # how much of the most common words one should take\n",
    "vocab = set()\n",
    "for cat_name in cats.all_names():\n",
    "    words = [w for w, f in vocabulary[cat_name].most_common(M)]\n",
    "    vocab = vocab.union(words)\n",
    "\n",
    "# words that appear in the most common words of more the #x=10 categories\n",
    "# are denoted as stoppwords.\n",
    "stopwords = set()\n",
    "for cat_names in itertools.combinations( cats.all_names(), 10):\n",
    "    cat_names = iter(cat_names)\n",
    "    sub_stops = set([w for w, f in vocabulary[next(cat_names)].most_common(M)])\n",
    "    for cat_name in cat_names:\n",
    "        sub_stops = sub_stops.intersection( set([w for w, f in vocabulary[cat_name].most_common(M)]) )\n",
    "    stopwords = stopwords.union(sub_stops)\n",
    "\n",
    "vocab = vocab.symmetric_difference(stopwords)\n",
    "\n",
    "feature_set = [(simple_features(vocab, q), cats.name(q['cat_id'])) for q in questions]\n",
    "train_set, test_set = feature_set[:10000], feature_set[10000:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\",\n",
       " \"'s\",\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '17',\n",
       " '19',\n",
       " '20',\n",
       " '22',\n",
       " '24',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " 'ab',\n",
       " 'abend',\n",
       " 'aber',\n",
       " 'abkurz',\n",
       " 'abnehm',\n",
       " 'absend',\n",
       " 'abspritz',\n",
       " 'adress',\n",
       " 'aktivi',\n",
       " 'aktuell',\n",
       " 'album',\n",
       " 'alexandra',\n",
       " 'alfa',\n",
       " 'alina',\n",
       " 'all',\n",
       " 'also',\n",
       " 'anal',\n",
       " 'analsex',\n",
       " 'and',\n",
       " 'andreas',\n",
       " 'anfang',\n",
       " 'angemeldet',\n",
       " 'anna',\n",
       " 'annika',\n",
       " 'antwort',\n",
       " 'arab',\n",
       " 'ashley',\n",
       " 'attack',\n",
       " 'aug',\n",
       " 'auto',\n",
       " 'autogramm',\n",
       " 'autogrammadress',\n",
       " 'bahn',\n",
       " 'bahnhof',\n",
       " 'banan',\n",
       " 'band',\n",
       " 'bay',\n",
       " 'beantwort',\n",
       " 'beckham',\n",
       " 'bedeudet',\n",
       " 'bedeut',\n",
       " 'bedeuted',\n",
       " 'bedeutet',\n",
       " 'befried',\n",
       " 'begriff',\n",
       " 'beim',\n",
       " 'bekommt',\n",
       " 'beliebt',\n",
       " 'berg',\n",
       " 'berlin',\n",
       " 'besiegt',\n",
       " 'bess',\n",
       " 'best',\n",
       " 'besteh',\n",
       " 'besteht',\n",
       " 'beteutet',\n",
       " 'bezeichnet',\n",
       " 'bezieh',\n",
       " 'bianca',\n",
       " 'bieb',\n",
       " 'bill',\n",
       " 'bin',\n",
       " 'bioshock',\n",
       " 'bis',\n",
       " 'bitt',\n",
       " 'bizarr',\n",
       " 'bleib',\n",
       " 'blue',\n",
       " 'blut',\n",
       " 'borussia',\n",
       " 'box',\n",
       " 'brauch',\n",
       " 'braucht',\n",
       " 'bravo',\n",
       " 'brem',\n",
       " 'bros',\n",
       " 'broth',\n",
       " 'brust',\n",
       " 'buch',\n",
       " 'buchstab',\n",
       " 'bundesliga',\n",
       " 'bushido',\n",
       " 'bzw',\n",
       " 'call',\n",
       " 'cd',\n",
       " 'champion',\n",
       " 'cheat',\n",
       " 'chines',\n",
       " 'christiano',\n",
       " 'cinema',\n",
       " 'city',\n",
       " 'cm',\n",
       " 'cod',\n",
       " 'comput',\n",
       " 'cristiano',\n",
       " 'cyrus',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'daniel',\n",
       " 'dank',\n",
       " 'dann',\n",
       " 'darf',\n",
       " 'dass',\n",
       " 'dauert',\n",
       " 'david',\n",
       " 'de',\n",
       " 'definiert',\n",
       " 'dein',\n",
       " 'denn',\n",
       " 'dennis',\n",
       " 'des',\n",
       " 'deutsch',\n",
       " 'deutschland',\n",
       " 'diablo',\n",
       " 'diana',\n",
       " 'dich',\n",
       " 'dick',\n",
       " 'dies',\n",
       " 'direkt',\n",
       " 'dortmund',\n",
       " 'download',\n",
       " 'driv',\n",
       " 'ds',\n",
       " 'dsds',\n",
       " 'dsl',\n",
       " 'du',\n",
       " 'dud',\n",
       " 'durch',\n",
       " 'durchschnitt',\n",
       " 'duty',\n",
       " 'dvd',\n",
       " 'efron',\n",
       " 'ei',\n",
       " 'eier',\n",
       " 'eigenschaft',\n",
       " 'eigent',\n",
       " 'einig',\n",
       " 'einwohn',\n",
       " 'elf',\n",
       " 'emo',\n",
       " 'end',\n",
       " 'englisch',\n",
       " 'entstand',\n",
       " 'entsteh',\n",
       " 'entsteht',\n",
       " 'erd',\n",
       " 'erfund',\n",
       " 'erkennt',\n",
       " 'erst',\n",
       " 'ess',\n",
       " 'euro',\n",
       " 'ex',\n",
       " 'facebook',\n",
       " 'fahr',\n",
       " 'fahrt',\n",
       " 'fahrzeug',\n",
       " 'famili',\n",
       " 'familiennam',\n",
       " 'fan',\n",
       " 'fantasy',\n",
       " 'fc',\n",
       " 'fernseh',\n",
       " 'feucht',\n",
       " 'feuerwehr',\n",
       " 'fick',\n",
       " 'fifa',\n",
       " 'film',\n",
       " 'final',\n",
       " 'find',\n",
       " 'findet',\n",
       " 'fisch',\n",
       " 'flieg',\n",
       " 'florian',\n",
       " 'folg',\n",
       " 'for',\n",
       " 'frag',\n",
       " 'frank',\n",
       " 'frankfurt',\n",
       " 'franziska',\n",
       " 'franzos',\n",
       " 'frau',\n",
       " 'frequenz',\n",
       " 'freund',\n",
       " 'freundin',\n",
       " 'fuhlt',\n",
       " 'fussball',\n",
       " 'fussballspiel',\n",
       " 'fussballtor',\n",
       " 'fussballverein',\n",
       " 'gab',\n",
       " 'gaga',\n",
       " 'gam',\n",
       " 'geb',\n",
       " 'gebor',\n",
       " 'geburstag',\n",
       " 'geburtstag',\n",
       " 'gedicht',\n",
       " 'geg',\n",
       " 'geh',\n",
       " 'gehort',\n",
       " 'geht',\n",
       " 'geil',\n",
       " 'gekauft',\n",
       " 'geld',\n",
       " 'gemacht',\n",
       " 'genau',\n",
       " 'gerad',\n",
       " 'geschoss',\n",
       " 'geschrieb',\n",
       " 'geschwind',\n",
       " 'gespielt',\n",
       " 'gestorb',\n",
       " 'gesung',\n",
       " 'gewann',\n",
       " 'gewinn',\n",
       " 'gewinnt',\n",
       " 'gewonn',\n",
       " 'gezog',\n",
       " 'gi',\n",
       " 'glucklich',\n",
       " 'gold',\n",
       " 'gomez',\n",
       " 'googl',\n",
       " 'gott',\n",
       " 'grad',\n",
       " 'gran',\n",
       " 'gross',\n",
       " 'grosst',\n",
       " 'grupp',\n",
       " 'gta',\n",
       " 'guru',\n",
       " 'gut',\n",
       " 'haar',\n",
       " 'hallo',\n",
       " 'hamburg',\n",
       " 'handy',\n",
       " 'handynumm',\n",
       " 'harry',\n",
       " 'hatt',\n",
       " 'hauptmenu',\n",
       " 'hauptstadt',\n",
       " 'haus',\n",
       " 'haut',\n",
       " 'heik',\n",
       " 'heirat',\n",
       " 'heiss',\n",
       " 'heist',\n",
       " 'her',\n",
       " 'heut',\n",
       " 'hiess',\n",
       " 'hitl',\n",
       " 'hoch',\n",
       " 'hotel',\n",
       " 'hubschraub',\n",
       " 'huhn',\n",
       " 'hund',\n",
       " 'icq',\n",
       " 'ihm',\n",
       " 'ihn',\n",
       " 'imm',\n",
       " 'ins',\n",
       " 'internet',\n",
       " 'internetseit',\n",
       " 'ja',\n",
       " 'jackson',\n",
       " 'jahr',\n",
       " 'jahrestag',\n",
       " 'jahreszahl',\n",
       " 'jan',\n",
       " 'janin',\n",
       " 'japan',\n",
       " 'jasmin',\n",
       " 'jed',\n",
       " 'jemal',\n",
       " 'jemand',\n",
       " 'jennif',\n",
       " 'jessica',\n",
       " 'jetzt',\n",
       " 'jugend',\n",
       " 'julia',\n",
       " 'jung',\n",
       " 'jungfrau',\n",
       " 'justin',\n",
       " 'kalori',\n",
       " 'kan',\n",
       " 'kart',\n",
       " 'katharina',\n",
       " 'katz',\n",
       " 'kauf',\n",
       " 'kein',\n",
       " 'kenn',\n",
       " 'kevin',\n",
       " 'kg',\n",
       " 'kind',\n",
       " 'kino',\n",
       " 'kinos',\n",
       " 'klein',\n",
       " 'klick',\n",
       " 'koln',\n",
       " 'komm',\n",
       " 'kommend',\n",
       " 'konn',\n",
       " 'konnt',\n",
       " 'konsol',\n",
       " 'korbchengross',\n",
       " 'korp',\n",
       " 'kostenlos',\n",
       " 'kostet',\n",
       " 'krankheit',\n",
       " 'kreb',\n",
       " 'krieg',\n",
       " 'kriegt',\n",
       " 'lad',\n",
       " 'lady',\n",
       " 'land',\n",
       " 'landkreis',\n",
       " 'lang',\n",
       " 'larissa',\n",
       " 'lass',\n",
       " 'latein',\n",
       " 'lauft',\n",
       " 'laura',\n",
       " 'laut',\n",
       " 'lautet',\n",
       " 'lea',\n",
       " 'leb',\n",
       " 'lebt',\n",
       " 'lesb',\n",
       " 'letzt',\n",
       " 'level',\n",
       " 'lieb',\n",
       " 'liebt',\n",
       " 'lied',\n",
       " 'liegt',\n",
       " 'liga',\n",
       " 'lisa',\n",
       " 'lit',\n",
       " 'lol',\n",
       " 'los',\n",
       " 'lotozahl',\n",
       " 'lotto',\n",
       " 'lottozahl',\n",
       " 'low',\n",
       " 'lvl',\n",
       " 'mach',\n",
       " 'macht',\n",
       " 'madch',\n",
       " 'mag',\n",
       " 'maik',\n",
       " 'mail',\n",
       " 'mal',\n",
       " 'mandy',\n",
       " 'manga',\n",
       " 'manhunt',\n",
       " 'mann',\n",
       " 'mannlich',\n",
       " 'mannschaft',\n",
       " 'maoam',\n",
       " 'map',\n",
       " 'marcel',\n",
       " 'mari',\n",
       " 'marina',\n",
       " 'mario',\n",
       " 'markus',\n",
       " 'masturbi',\n",
       " 'matthias',\n",
       " 'meer',\n",
       " 'mehr',\n",
       " 'mehrzahl',\n",
       " 'meist',\n",
       " 'melani',\n",
       " 'mell',\n",
       " 'mensch',\n",
       " 'mert',\n",
       " 'mich',\n",
       " 'michael',\n",
       " 'michaela',\n",
       " 'microsoft',\n",
       " 'miley',\n",
       " 'mittwoch',\n",
       " 'mmorpg',\n",
       " 'mobil',\n",
       " 'mod',\n",
       " 'mond',\n",
       " 'mont',\n",
       " 'morg',\n",
       " 'mr',\n",
       " 'mtv',\n",
       " 'munch',\n",
       " 'muschi',\n",
       " 'musik',\n",
       " 'muss',\n",
       " 'mutt',\n",
       " 'nachfolg',\n",
       " 'nachnam',\n",
       " 'nacht',\n",
       " 'nackt',\n",
       " 'nadin',\n",
       " 'nah',\n",
       " 'nahm',\n",
       " 'nam',\n",
       " 'namenstag',\n",
       " 'namm',\n",
       " 'need',\n",
       " 'nehm',\n",
       " 'nennt',\n",
       " 'net',\n",
       " 'neu',\n",
       " 'new',\n",
       " 'nico',\n",
       " 'nina',\n",
       " 'nintendo',\n",
       " 'normal',\n",
       " 'numm',\n",
       " 'nur',\n",
       " 'nurnberg',\n",
       " 'ob',\n",
       " 'of',\n",
       " 'oft',\n",
       " 'oh',\n",
       " 'ohn',\n",
       " 'oliv',\n",
       " 'onani',\n",
       " 'onlin',\n",
       " 'orgasmus',\n",
       " 'ort',\n",
       " 'paris',\n",
       " 'park',\n",
       " 'pass',\n",
       " 'passiert',\n",
       " 'passt',\n",
       " 'passwort',\n",
       " 'pat',\n",
       " 'patrick',\n",
       " 'pc',\n",
       " 'penis',\n",
       " 'pet',\n",
       " 'pi',\n",
       " 'pickel',\n",
       " 'planet',\n",
       " 'playstation',\n",
       " 'pokemon',\n",
       " 'porno',\n",
       " 'pornos',\n",
       " 'pro',\n",
       " 'promi',\n",
       " 'promis',\n",
       " 'prozent',\n",
       " 'ps',\n",
       " 'ps2',\n",
       " 'ps3',\n",
       " 'psp',\n",
       " 'pups',\n",
       " 'ramona',\n",
       " 'rapp',\n",
       " 'rauch',\n",
       " 'raus',\n",
       " 'rebecca',\n",
       " 'redewend',\n",
       " 'reich',\n",
       " 'richtig',\n",
       " 'robert',\n",
       " 'rock',\n",
       " 'roman',\n",
       " 'ronaldo',\n",
       " 'ronny',\n",
       " 'rtl',\n",
       " 'rund',\n",
       " 'sabrina',\n",
       " 'sag',\n",
       " 'sagt',\n",
       " 'saison',\n",
       " 'samstag',\n",
       " 'san',\n",
       " 'sandra',\n",
       " 'sang',\n",
       " 'sarah',\n",
       " 'satansbrat',\n",
       " 'schalk',\n",
       " 'schauspiel',\n",
       " 'schauspielerin',\n",
       " 'scheid',\n",
       " 'schluss',\n",
       " 'schnell',\n",
       " 'schoss',\n",
       " 'schrieb',\n",
       " 'schuh',\n",
       " 'schul',\n",
       " 'schulervz',\n",
       " 'schutz',\n",
       " 'schutzengel',\n",
       " 'schwang',\n",
       " 'schwanz',\n",
       " 'schwarz',\n",
       " 'schweinegripp',\n",
       " 'schwer',\n",
       " 'schwerst',\n",
       " 'schwul',\n",
       " 'sehr',\n",
       " 'seit',\n",
       " 'sekund',\n",
       " 'selb',\n",
       " 'selena',\n",
       " 'send',\n",
       " 'sendung',\n",
       " 'sephiroth',\n",
       " 'seri',\n",
       " 'sex',\n",
       " 'sg',\n",
       " 'sieg',\n",
       " 'sieht',\n",
       " 'sim',\n",
       " 'simon',\n",
       " 'simpson',\n",
       " 'singt',\n",
       " 'sinn',\n",
       " 'skorpion',\n",
       " 'smash',\n",
       " 'sms',\n",
       " 'smsguru',\n",
       " 'so',\n",
       " 'softwar',\n",
       " 'soll',\n",
       " 'song',\n",
       " 'sonn',\n",
       " 'spanisch',\n",
       " 'spear',\n",
       " 'speed',\n",
       " 'sperma',\n",
       " 'spiel',\n",
       " 'spielt',\n",
       " 'spongebob',\n",
       " 'sportart',\n",
       " 'sprach',\n",
       " 'spruch',\n",
       " 'stadion',\n",
       " 'stadt',\n",
       " 'staffel',\n",
       " 'stalk',\n",
       " 'stamm',\n",
       " 'stammt',\n",
       " 'star',\n",
       " 'starb',\n",
       " 'stark',\n",
       " 'statt',\n",
       " 'steh',\n",
       " 'steht',\n",
       " 'steif',\n",
       " 'steinbock',\n",
       " 'stell',\n",
       " 'stellung',\n",
       " 'sterb',\n",
       " 'stern',\n",
       " 'sternzeich',\n",
       " 'stev',\n",
       " 'stier',\n",
       " 'strass',\n",
       " 'stuttgart',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'suss',\n",
       " 'sv',\n",
       " 'sven',\n",
       " 'tag',\n",
       " 'taylor',\n",
       " 'teil',\n",
       " 'tel',\n",
       " 'telefonnumm',\n",
       " 'test',\n",
       " 'testament',\n",
       " 'teu',\n",
       " 'the',\n",
       " 'thomas',\n",
       " 'tief',\n",
       " 'tier',\n",
       " 'tim',\n",
       " 'tisdal',\n",
       " 'tokio',\n",
       " 'tor',\n",
       " 'train',\n",
       " 'treff',\n",
       " 'tsv',\n",
       " 'tun',\n",
       " 'turismo',\n",
       " 'turkisch',\n",
       " 'tut',\n",
       " 'tv',\n",
       " 'twilight',\n",
       " 'uber',\n",
       " 'ubersetz',\n",
       " 'ubersetzt',\n",
       " 'uhr',\n",
       " 'unbeliebt',\n",
       " 'undertak',\n",
       " 'universum',\n",
       " 'unlimeted',\n",
       " 'uns',\n",
       " 'unt',\n",
       " 'unterschied',\n",
       " 'urin',\n",
       " 'url',\n",
       " 'ursprung',\n",
       " 'usa',\n",
       " 'vagina',\n",
       " 'vanessa',\n",
       " 'variant',\n",
       " 'verdient',\n",
       " 'verein',\n",
       " 'verheiratet',\n",
       " 'verliebt',\n",
       " 'version',\n",
       " 'versteht',\n",
       " 'vfb',\n",
       " 'vfl',\n",
       " 'vic',\n",
       " 'video',\n",
       " 'videos',\n",
       " 'vista',\n",
       " 'viva',\n",
       " 'vivi',\n",
       " 'voll',\n",
       " 'vom',\n",
       " 'vor',\n",
       " 'vornam',\n",
       " 'vorwahl',\n",
       " 'waag',\n",
       " 'wan',\n",
       " 'warcraft',\n",
       " 'warfar',\n",
       " 'warm',\n",
       " 'wass',\n",
       " 'wassermann',\n",
       " 'watch',\n",
       " 'websit',\n",
       " 'weh',\n",
       " 'weiblich',\n",
       " 'weiss',\n",
       " 'weit',\n",
       " 'welt',\n",
       " 'weltkrieg',\n",
       " 'weltweit',\n",
       " 'wem',\n",
       " 'wen',\n",
       " 'werbung',\n",
       " 'wett',\n",
       " 'widd',\n",
       " 'wied',\n",
       " 'wieg',\n",
       " 'wiegt',\n",
       " 'wieso',\n",
       " 'wii',\n",
       " 'will',\n",
       " 'windows',\n",
       " 'wir',\n",
       " 'wirklich',\n",
       " 'wiss',\n",
       " 'witz',\n",
       " 'wm',\n",
       " 'woch',\n",
       " 'wofur',\n",
       " 'woh',\n",
       " 'wohn',\n",
       " 'wohnt',\n",
       " 'wollsock',\n",
       " 'woran',\n",
       " 'world',\n",
       " 'wort',\n",
       " 'wow',\n",
       " 'wrestl',\n",
       " 'wrestling',\n",
       " 'wurd',\n",
       " 'wurfel',\n",
       " 'wwe',\n",
       " 'www',\n",
       " 'xbox360',\n",
       " 'youtub',\n",
       " 'yu',\n",
       " 'yvonn',\n",
       " 'zac',\n",
       " 'zahl',\n",
       " 'zeit',\n",
       " 'zuerst',\n",
       " 'zug',\n",
       " 'zukunft',\n",
       " 'zur',\n",
       " 'zuruck',\n",
       " 'zusamm',\n",
       " 'zuzumau',\n",
       " 'zweit',\n",
       " 'zwilling',\n",
       " 'zwisch'}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), len(stopwords)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wissen'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = {}\n",
    "example['words'] = word_tokenize(\"Wie heißt der Sänger  von U2\".lower())\n",
    "example\n",
    "cats.name(questions[q_id]['cat_id']), classifier.classify(simple_features(vocab, questions[q_id]))\n",
    "classifier.classify(simple_features(vocab,example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.560208239022182"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-c6cb8f4412ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-155-c6cb8f4412ae>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mprob_classify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mfeature_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mlogprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfeature_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;31m# nb: This case will never come up if the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# Default definition, in terms of prob()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_NINF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_divisor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = [classifier.classify(q) for q, _ in test_set]\n",
    "tes = [c for _, c in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res[:10],tes[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words is 126685. \n",
      "The volume of the vocabulary is 18236.\n",
      "That makes an percentiage of 0.14\n"
     ]
    }
   ],
   "source": [
    "totNum = vocabulary.N()\n",
    "vocNum = vocabulary.B()\n",
    "print(\"\"\"The total number of words is {0}. \n",
    "The volume of the vocabulary is {1}.\n",
    "That makes an percentiage of {2:.2}\"\"\".format(totNum,vocNum,vocNum/totNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allwordlist = [w for q in questions for w in q[1]]\n",
    "fd = nltk.FreqDist(allwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ? x 11576   bedeutet x 1465       kommt x 947           '' x 649         oder x 506   \n",
      "       der x 4693         das x 1386          wo x 879       welche x 642         mein x 481   \n",
      "       wie x 3999         hat x 1351         wer x 810         gibt x 593          bei x 478   \n",
      "       was x 3919         von x 1260        eine x 781          den x 587         viel x 455   \n",
      "       ist x 3470         ein x 1135         mit x 769           er x 574         wenn x 452   \n",
      "       die x 2366          es x 1112        kann x 749          für x 562         welt x 430   \n",
      "       und x 2255         man x 1075       woher x 728        heißt x 554          sie x 423   \n",
      "       ich x 2011           , x 1061         auf x 701         sind x 548        haben x 415   \n",
      "        in x 1717           . x 1004       warum x 687           im x 542        viele x 415   \n",
      "      name x 1652        wann x 997           am x 683           zu x 530          aus x 380   \n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "delta = 0\n",
    "rows = 5\n",
    "epc = int(n/rows)\n",
    "a = np.arange(n) + delta\n",
    "\n",
    "for j in range(epc):\n",
    "    row = [fd.most_common(n)[i] for i in a[j::epc]]\n",
    "    for v in row:\n",
    "        print((\"{0:>10} x {1:<6}\").format(*v),end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- anschlag            - anstecken           - antikatalytische    - antonin             - antwortetst         \n",
      "- anschliesend        - ansteckend          - antike              - antonio             - anubis              \n",
      "- anschließen         - anstellen           - antimaterie         - antony              - anunis              \n",
      "- anschlägt           - antarktis           - antipinoxe          - antreten            - anus                \n",
      "- anschrift           - anteil              - antisemetismus      - antrieb             - anwalt              \n",
      "- anschwillt          - antenne             - antisemitismus      - antrittsvorlesung   - anwendungen         \n",
      "- ansehen             - antennenverhältnis  - antje               - anträge             - anwendungsgebiete   \n",
      "- ansprechen          - anthony             - anton               - antwort             - anwesen             \n",
      "- anspruch            - antibabypille       - antonella           - antworten           - anwesend            \n",
      "- anstatt             - antidepressiva      - antonia             - antwortet           - anwort              \n"
     ]
    }
   ],
   "source": [
    "vocabArr = sorted(list(vocab))\n",
    "\n",
    "n = 50\n",
    "delta = 2000\n",
    "rows = 5\n",
    "epc = int(n/rows)\n",
    "a = np.arange(n) + delta\n",
    "\n",
    "for j in range(epc):\n",
    "    row = [vocabArr[i] for i in a[j::epc]]\n",
    "    print((\"- {:20}\"*len(row)).format(*row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Was']\n",
      "['WIEVIEL']\n",
      "['Was']\n",
      "[]\n",
      "[]\n",
      "['Wie']\n",
      "['Wie']\n",
      "['Wie']\n",
      "[]\n",
      "['Was']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for q in questions[:10]:\n",
    "    print([w for w in q[1] if re.search('^W',w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001 1     1    \n",
      "00002 4     8    \n",
      "00003 9     27   \n",
      "00004 16    64   \n",
      "00005 25    125  \n",
      "00006 36    216  \n",
      "00007 49    343  \n",
      "00008 64    512  \n",
      "00009 81    729  \n",
      "00010 100   1000 \n"
     ]
    }
   ],
   "source": [
    "for x in range(1,11):\n",
    "    print(repr(x).zfill(5), repr(x**2).ljust(5),repr(x**3).ljust(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(names.words('male.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48,\n",
       "        51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99]),\n",
       " array([99,  2,  5,  8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47,\n",
       "        50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98]),\n",
       " array([98,  1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46,\n",
       "        49, 52, 55, 58, 61, 64, 67, 70, 73, 76, 79, 82, 85, 88, 91, 94, 97])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(100)\n",
    "[b[a[::3]-i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 'Cat'), ('house', 'Dog'), ('dor', 'Mouse')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['car', 'house', 'dor']\n",
    "b = ['Cat', 'Dog', 'Mouse']\n",
    "list(zip(*[a,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking for ill-shaped lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 16070\n",
      "Number of rows with wrong length: 57:\n",
      "    fraction: 0.35 %\n"
     ]
    }
   ],
   "source": [
    "csvfile.seek(0);\n",
    "i = 0 #count total row number\n",
    "j = 0 #count false row number\n",
    "\n",
    "for row in reader:\n",
    "    i += 1\n",
    "    if(len(row) != 21):\n",
    "        #print('line size error in line {} \\n'.format(reader.line_num))\n",
    "        #print('line size is: {} \\n'.format(len(row)))\n",
    "        #print(*row)\n",
    "        #print('\\n')\n",
    "        j += 1\n",
    "\n",
    "print(\"\"\"Total number of rows: {0}\n",
    "Number of rows with wrong length: {1}:\n",
    "    fraction: {2:.2f} %\"\"\".format(i,j,(j/i)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['      ', '  ', '      ', '  ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\" {2,}\", \"Paul      Hager  er      hat  kein Geld.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lower' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d9db7e7a7284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dfsdfadSADSFDF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lower' is not defined"
     ]
    }
   ],
   "source": [
    "lower(\"dfsdfadSADSFDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = snowball.GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hallo',\n",
       " 'mein',\n",
       " 'Name',\n",
       " 'ist',\n",
       " 'Paul',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'jahreszahl',\n",
       " 'gebohren']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(filters)\n",
    "sentence = \"Hallo mein Name ist Paul ich bin 1992 gebohren\"\n",
    "filters.year_tracker(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jahreszahl'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"^[1][89][0-9]{2}$\",\"jahreszahl\", \"1993\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
