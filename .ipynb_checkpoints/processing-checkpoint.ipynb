{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "import pickle\n",
    "from categories import categories\n",
    "import filters\n",
    "import importlib\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: question_id               1: user_id                   2: sms_guru_id               3: category_main_id         \n",
      " 4: question                  5: description               6: tags                      7: categories               \n",
      " 8: url                       9: rating_count_positive    10: rating_count_negative    11: answer_count             \n",
      "12: reported                 13: answered                 14: active                   15: deleted                  \n",
      "16: seo_locked               17: editor_locked            18: editor_id                19: created_at               \n",
      "20: updated_at               \n",
      " 0: 5105                      1: 0                         2: 70470                    \n",
      " 3: 42                        4: Welche dermatologische funktion hat sonnenbumenöl für die haut 5:                           6: sonnenblumenöl, haut, funktion\n",
      " 7: Gesundheit                8: mensch-koerper/gesundheit/welche-dermatologische-funktion-hat-sonnenbumenoel-fuer-die-haut 9: 0                        10: 0                        \n",
      "11: 0                        12: 0                        13: 0                        14: 1                        \n",
      "15: 0                        16: 1                        17: 0000-00-00 00:00:00      18: 10                       \n",
      "19: 2010-04-05 21:40:35      20: 2010-04-08 20:49:54      "
     ]
    }
   ],
   "source": [
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "\n",
    "#cols = list(zip(np.arange(21),next(reader)))\n",
    "j = 0\n",
    "for i, q in zip(np.arange(21), next(qreader)):\n",
    "    if j == 3: l = \"\\n\"; j = 0;\n",
    "    else: l = \"\"; j += 1\n",
    "        \n",
    "    print('{0:2}: {1:25}'.format(i,q), end=l)\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "qes = next(qreader)\n",
    "i = 0\n",
    "while i < 100:\n",
    "    qes =next(qreader)\n",
    "    if qes[16] != \"0\":\n",
    "        i += 1\n",
    "for i, q in zip(np.arange(21), qes):\n",
    "    if j == 3: l = \"\\n\"; j = 0;\n",
    "    else: l = \"\"; j += 1\n",
    "        \n",
    "    print('{0:2}: {1:25}'.format(i,q), end=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading out questions and tokenizing, checking vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wissen', 2266),\n",
       " ('namensforschung', 2165),\n",
       " ('stars_and_promis', 1840),\n",
       " ('schule', 1291),\n",
       " ('freizeit_and_sport', 1244),\n",
       " ('mensch_and_koerper', 1184),\n",
       " ('film_and_musik', 823),\n",
       " ('literatur_and_sprache', 753),\n",
       " ('liebe_and_beziehung', 728),\n",
       " ('adult', 718),\n",
       " ('alltag', 569),\n",
       " ('astrologie', 511),\n",
       " ('games_and_spiele', 192),\n",
       " ('computer_and_pc', 170)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = categories()\n",
    "\n",
    "qcatfile = open('question_category_train.csv', 'r')\n",
    "qcatreader = csv.reader(qcatfile)\n",
    "\n",
    "next(qcatreader) # skipping column discription\n",
    "\n",
    "qcat_dict = {} # mapping from question_id to the parent category_id\n",
    "cat_freq = nltk.FreqDist() # maps the category_id to its\n",
    "\n",
    "for qcat in qcatreader:\n",
    "    cat_id = int(qcat[1])\n",
    "    pcat_id = cats.parent_id(cat_id)\n",
    "    q_id = int(qcat[2])\n",
    "    \n",
    "    qcat_dict[q_id] = pcat_id\n",
    "    cat_freq[cats.name(cat_id)] += 1\n",
    "    \n",
    "#cat_freq.plot()\n",
    "cat_freq.most_common(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1d515004a1af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcats_new_ids\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mcats_new_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msub_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cats' is not defined"
     ]
    }
   ],
   "source": [
    "cats_new_ids ={}\n",
    "for cat, i in zip( cats.all_names(), range(14) ):\n",
    "    cats_new_ids[cat] = i\n",
    "\n",
    "sub_count = -np.ones(14)\n",
    "for subcat in cats.subcats.keys():\n",
    "    sub_count[cats_new_ids[cats.name(subcat)]] += 1\n",
    "    \n",
    "max(sub_count),min(sub_count),sum(sub_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(filters)\n",
    "\n",
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "next(qreader)\n",
    "\n",
    "questions = []\n",
    "vocabulary = {}\n",
    "vocabulary['all'] = nltk.FreqDist()\n",
    "for cat_name in cats.all_names(): vocabulary[cat_name] = nltk.FreqDist()\n",
    "\n",
    "# Set this parameter to TRUE if you want to read through\n",
    "# all questions again, elsewise from file set to FALSE.\n",
    "NewRead = True \n",
    "\n",
    "total_enteties_count = 0\n",
    "\n",
    "if NewRead:\n",
    "    \n",
    "    for row in qreader:\n",
    "        if len(row) != 21: continue\n",
    "        if row[15] != \"0\": continue\n",
    "            \n",
    "        if int(row[0]) in qcat_dict.keys():\n",
    "            cat_id = qcat_dict[int(row[0])]\n",
    "            \n",
    "            sentence = row[4].lower()\n",
    "            # running a sequence of filters on the raw question string \n",
    "            for filt in [filters.punctuation_filter]:\n",
    "                sentence = filt(sentence)\n",
    "            \n",
    "            words = word_tokenize(sentence)\n",
    "            # running a sequence of filtes on the already tokenized sentence\n",
    "            for filt in [filters.year_tracker, filters.small_word_filter, filters.stemming_filter]:\n",
    "                words = filt(words)\n",
    "            \n",
    "            questions += [{\"words\": words, \"cat_id\": cat_id}]\n",
    "            vocabulary[ cats.name(cat_id) ] += nltk.FreqDist(words)\n",
    "            vocabulary['all'] += nltk.FreqDist(words)\n",
    "        \n",
    "    ## Saving into pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'wb'), open('vocabulary.pkl', 'wb')\n",
    "    pickle.dump(questions, q_file)\n",
    "    pickle.dump(vocabulary, v_file)\n",
    "    \n",
    "else:\n",
    "    ## Loading from pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'rb'), open('vocabulary.pkl', 'rb')\n",
    "    questions, vocabulary = pickle.load(q_file), pickle.load(v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.85024623708 4.62479783935 112 0 14417\n"
     ]
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "a = np.array([len(q['words']) for q in questions])\n",
    "print(a.mean(), a.std(), a.max(), a.min(), len(a))\n",
    "\n",
    "plt.hist(a[a<30])\n",
    "plt.title(\"Question length distiribution\")\n",
    "plt.xlabel(\"#Words in question\")\n",
    "plt.ylabel(\"#Question\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film_and_musik', 'stars_and_promis', 'computer_and_pc', 'alltag', 'namensforschung', 'literatur_and_sprache', 'schule', 'mensch_and_koerper', 'freizeit_and_sport', 'wissen', 'liebe_and_beziehung', 'astrologie', 'games_and_spiele', 'adult']\n"
     ]
    }
   ],
   "source": [
    "print(cats.all_names())\n",
    "#print(vocabulary['computer_and_pc'].most_common(20))\n",
    "#print(list(vocabulary['computer_and_pc'])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_features(vocab, question):\n",
    "    features = {}\n",
    "    for v in vocab:\n",
    "        features[v] = v in question['words']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## information gain based vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14640 of 14640) |###################| Elapsed Time: 0:03:30 Time: 0:03:30\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "information_gain = {}\n",
    "\n",
    "calculate_new = False\n",
    "\n",
    "if caluculate_new:\n",
    "    bar = progressbar.ProgressBar()\n",
    "    for w in bar(vocabulary['all']):\n",
    "        for cat in cats.all_names():\n",
    "            information_gain[(w, cat)] = 0\n",
    "            if vocabulary[cat].freq(w) > 0:\n",
    "                information_gain[(w, cat)] = vocabulary[cat].freq(w) * cat_freq.freq(cat) \\\n",
    "                                        * np.log( vocabulary[cat].freq(w) / vocabulary['all'].freq(w) )\n",
    "            if vocabulary[cat].freq(w) < 1:\n",
    "                information_gain[(w, cat)] += (1 - vocabulary[cat].freq(w)) * cat_freq.freq(cat) \\\n",
    "                                        * np.log( (1 - vocabulary[cat].freq(w)) / (1 -vocabulary['all'].freq(w)) )\n",
    "    ## Saving into pickle files\n",
    "    ig_file = open('information_gain.pkl', 'wb+')\n",
    "    pickle.dump(information_gain, ig_file)\n",
    "else:\n",
    "    else:\n",
    "    ## Loading from pickle files\n",
    "    ig_file = open('information_gain.pkl', 'rb')\n",
    "    ig_file = pickle.load(ig_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1217"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import progressbar\n",
    "# ten best words by information_gain\n",
    "vocab = set()\n",
    "\n",
    "bar = progressbar.ProgressBar()\n",
    "for cat in bar(cats.all_names()):\n",
    "    lis = [(information_gain[(w,cat)], w) for w in vocabulary['all']]\n",
    "    lis.sort()\n",
    "    vocab = vocab.union([w for _, w in lis[-150:]])\n",
    "    \n",
    "ig_based_vocab = vocab\n",
    "len(ig_based_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## most common based vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 220 # how much of the most common words one should take\n",
    "vocab = set()\n",
    "for cat_name in cats.all_names():\n",
    "    words = [w for w,_ in vocabulary[cat_name].most_common(M)]\n",
    "    vocab = vocab.union(words)\n",
    "\n",
    "most_common_vocab = vocab\n",
    "len(most_common_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1215, 'sopwords reduced', 45)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = 227 # how much of the most common words one should take\n",
    "S, MS = 10, 100 # if a word appears in the #MS most common words of #S documents its sorted out\n",
    "vocab = set()\n",
    "for cat_name in cats.all_names():\n",
    "    words = [w for w,_ in vocabulary[cat_name].most_common(M)]\n",
    "    vocab = vocab.union(words)\n",
    "\n",
    "# words that appear in the most common words of more the #x=8 categories\n",
    "# are denoted as stoppwords.\n",
    "stopwords = set()\n",
    "for cat_names in itertools.combinations( cats.all_names(), S):\n",
    "    cat_names = iter(cat_names)\n",
    "    sub_stops = set([w for w, f in vocabulary[next(cat_names)].most_common(MS)])\n",
    "    for cat_name in cat_names:\n",
    "        sub_stops = sub_stops.intersection( set([w for w, f in vocabulary[cat_name].most_common(MS)]) )\n",
    "    stopwords = stopwords.union(sub_stops)\n",
    "\n",
    "most_common_reduced_vocab = vocab.symmetric_difference(stopwords)\n",
    "len(most_common_reduced_vocab), \"sopwords reduced\", len(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Classifyers based on diffrent Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....training  ig_based classifier\n",
      "....training  most_common classifier\n",
      "....training  most_common_reduced classifier\n",
      "### TESTING CLASSIFIER ig_based ###\n",
      "vocabulary volume:  1217\n",
      "standart accurcy: 0.568\n",
      "uniform accuracy: 0.46457154415127794\n",
      "accuracy within category film_and_musik         : 0.41\n",
      "accuracy within category stars_and_promis       : 0.7\n",
      "accuracy within category computer_and_pc        : 0.12\n",
      "accuracy within category alltag                 : 0.2\n",
      "accuracy within category namensforschung        : 0.94\n",
      "accuracy within category literatur_and_sprache  : 0.3\n",
      "accuracy within category schule                 : 0.47\n",
      "accuracy within category mensch_and_koerper     : 0.45\n",
      "accuracy within category freizeit_and_sport     : 0.6\n",
      "accuracy within category wissen                 : 0.64\n",
      "accuracy within category liebe_and_beziehung    : 0.59\n",
      "accuracy within category astrologie             : 0.5\n",
      "accuracy within category games_and_spiele       : 0.31\n",
      "accuracy within category adult                  : 0.28\n",
      "\n",
      "### TESTING CLASSIFIER most_common ###\n",
      "vocabulary volume:  1213\n",
      "standart accurcy: 0.563\n",
      "uniform accuracy: 0.4541273140695225\n",
      "accuracy within category film_and_musik         : 0.36\n",
      "accuracy within category stars_and_promis       : 0.72\n",
      "accuracy within category computer_and_pc        : 0.12\n",
      "accuracy within category alltag                 : 0.17\n",
      "accuracy within category namensforschung        : 0.94\n",
      "accuracy within category literatur_and_sprache  : 0.29\n",
      "accuracy within category schule                 : 0.48\n",
      "accuracy within category mensch_and_koerper     : 0.44\n",
      "accuracy within category freizeit_and_sport     : 0.59\n",
      "accuracy within category wissen                 : 0.64\n",
      "accuracy within category liebe_and_beziehung    : 0.59\n",
      "accuracy within category astrologie             : 0.48\n",
      "accuracy within category games_and_spiele       : 0.27\n",
      "accuracy within category adult                  : 0.27\n",
      "\n",
      "### TESTING CLASSIFIER most_common_reduced ###\n",
      "vocabulary volume:  1215\n",
      "standart accurcy: 0.5675\n",
      "uniform accuracy: 0.4574429629233914\n",
      "accuracy within category film_and_musik         : 0.37\n",
      "accuracy within category stars_and_promis       : 0.73\n",
      "accuracy within category computer_and_pc        : 0.12\n",
      "accuracy within category alltag                 : 0.17\n",
      "accuracy within category namensforschung        : 0.94\n",
      "accuracy within category literatur_and_sprache  : 0.29\n",
      "accuracy within category schule                 : 0.47\n",
      "accuracy within category mensch_and_koerper     : 0.43\n",
      "accuracy within category freizeit_and_sport     : 0.6\n",
      "accuracy within category wissen                 : 0.65\n",
      "accuracy within category liebe_and_beziehung    : 0.59\n",
      "accuracy within category astrologie             : 0.48\n",
      "accuracy within category games_and_spiele       : 0.27\n",
      "accuracy within category adult                  : 0.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_set_size, te_set_size = 8000, 2000\n",
    "\n",
    "vocabularies = {\"ig_based\": ig_based_vocab, \n",
    "                \"most_common\": most_common_vocab,\n",
    "                \"most_common_reduced\": most_common_reduced_vocab}\n",
    "classifiers = {}\n",
    "\n",
    "# training calssifiers\n",
    "for vocab_key in vocabularies.keys():\n",
    "    print('....training ',vocab_key,'classifier')\n",
    "    vocab = vocabularies[vocab_key]\n",
    "    \n",
    "    feature_set = [(simple_features(vocab, q), cats.name(q['cat_id'])) for q in questions]\n",
    "    train_set, test_set = feature_set[:tr_set_size], feature_set[tr_set_size:tr_set_size + te_set_size]\n",
    "    \n",
    "    classifiers[vocab_key] = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# evaluating calssifiers\n",
    "for cf in classifiers.keys():\n",
    "    print(\"### TESTING CLASSIFIER\", cf, \"###\")\n",
    "    print(\"vocabulary volume: \", len(vocabularies[cf]))\n",
    "    \n",
    "    res = [classifiers[cf].classify(q) for q, _ in test_set]\n",
    "    indeed = [c for _, c in test_set]\n",
    "    cm = nltk.ConfusionMatrix(indeed, res)\n",
    "    \n",
    "    print(\"standart accurcy:\", standard_accuracy(cm))\n",
    "    print(\"uniform accuracy:\", uniform_accuracy(cm))\n",
    "    accuracy_list(cm)\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "#print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy_list(cm):\n",
    "    for cat in cats.all_names():\n",
    "        total_examples = sum([cm.__getitem__((cat, cat_)) for cat_ in cats.all_names()])\n",
    "        accuracy = cm.__getitem__((cat, cat)) / total_examples\n",
    "        print(\"accuracy within category {0:23}: {1:2.2}\".format(cat,accuracy))\n",
    "\n",
    "def uniform_accuracy(cm):\n",
    "    uni_accuracy = 0\n",
    "    for cat in cats.all_names():\n",
    "        total_examples = sum([cm.__getitem__((cat, cat_)) for cat_ in cats.all_names()])\n",
    "        accuracy = cm.__getitem__((cat, cat)) / total_examples\n",
    "        uni_accuracy += accuracy\n",
    "    return uni_accuracy / len(cats.all_names())\n",
    "\n",
    "def standard_accuracy(cm):\n",
    "    right_examples = 0\n",
    "    all_examples = 0\n",
    "    for cat in cats.all_names():\n",
    "        all_examples += sum([cm.__getitem__((cat, cat_)) for cat_ in cats.all_names()])\n",
    "        right_examples += cm.__getitem__((cat, cat))\n",
    "    return right_examples/all_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words is 126685. \n",
      "The volume of the vocabulary is 18236.\n",
      "That makes an percentiage of 0.14\n"
     ]
    }
   ],
   "source": [
    "totNum = vocabulary.N()\n",
    "vocNum = vocabulary.B()\n",
    "print(\"\"\"The total number of words is {0}. \n",
    "The volume of the vocabulary is {1}.\n",
    "That makes an percentiage of {2:.2}\"\"\".format(totNum,vocNum,vocNum/totNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allwordlist = [w for q in questions for w in q[1]]\n",
    "fd = nltk.FreqDist(allwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ? x 11576   bedeutet x 1465       kommt x 947           '' x 649         oder x 506   \n",
      "       der x 4693         das x 1386          wo x 879       welche x 642         mein x 481   \n",
      "       wie x 3999         hat x 1351         wer x 810         gibt x 593          bei x 478   \n",
      "       was x 3919         von x 1260        eine x 781          den x 587         viel x 455   \n",
      "       ist x 3470         ein x 1135         mit x 769           er x 574         wenn x 452   \n",
      "       die x 2366          es x 1112        kann x 749          für x 562         welt x 430   \n",
      "       und x 2255         man x 1075       woher x 728        heißt x 554          sie x 423   \n",
      "       ich x 2011           , x 1061         auf x 701         sind x 548        haben x 415   \n",
      "        in x 1717           . x 1004       warum x 687           im x 542        viele x 415   \n",
      "      name x 1652        wann x 997           am x 683           zu x 530          aus x 380   \n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "delta = 0\n",
    "rows = 5\n",
    "epc = int(n/rows)\n",
    "a = np.arange(n) + delta\n",
    "\n",
    "for j in range(epc):\n",
    "    row = [fd.most_common(n)[i] for i in a[j::epc]]\n",
    "    for v in row:\n",
    "        print((\"{0:>10} x {1:<6}\").format(*v),end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- anschlag            - anstecken           - antikatalytische    - antonin             - antwortetst         \n",
      "- anschliesend        - ansteckend          - antike              - antonio             - anubis              \n",
      "- anschließen         - anstellen           - antimaterie         - antony              - anunis              \n",
      "- anschlägt           - antarktis           - antipinoxe          - antreten            - anus                \n",
      "- anschrift           - anteil              - antisemetismus      - antrieb             - anwalt              \n",
      "- anschwillt          - antenne             - antisemitismus      - antrittsvorlesung   - anwendungen         \n",
      "- ansehen             - antennenverhältnis  - antje               - anträge             - anwendungsgebiete   \n",
      "- ansprechen          - anthony             - anton               - antwort             - anwesen             \n",
      "- anspruch            - antibabypille       - antonella           - antworten           - anwesend            \n",
      "- anstatt             - antidepressiva      - antonia             - antwortet           - anwort              \n"
     ]
    }
   ],
   "source": [
    "vocabArr = sorted(list(vocab))\n",
    "\n",
    "n = 50\n",
    "delta = 2000\n",
    "rows = 5\n",
    "epc = int(n/rows)\n",
    "a = np.arange(n) + delta\n",
    "\n",
    "for j in range(epc):\n",
    "    row = [vocabArr[i] for i in a[j::epc]]\n",
    "    print((\"- {:20}\"*len(row)).format(*row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Was']\n",
      "['WIEVIEL']\n",
      "['Was']\n",
      "[]\n",
      "[]\n",
      "['Wie']\n",
      "['Wie']\n",
      "['Wie']\n",
      "[]\n",
      "['Was']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for q in questions[:10]:\n",
    "    print([w for w in q[1] if re.search('^W',w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001 1     1    \n",
      "00002 4     8    \n",
      "00003 9     27   \n",
      "00004 16    64   \n",
      "00005 25    125  \n",
      "00006 36    216  \n",
      "00007 49    343  \n",
      "00008 64    512  \n",
      "00009 81    729  \n",
      "00010 100   1000 \n"
     ]
    }
   ],
   "source": [
    "for x in range(1,11):\n",
    "    print(repr(x).zfill(5), repr(x**2).ljust(5),repr(x**3).ljust(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(names.words('male.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48,\n",
       "        51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99]),\n",
       " array([99,  2,  5,  8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47,\n",
       "        50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98]),\n",
       " array([98,  1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46,\n",
       "        49, 52, 55, 58, 61, 64, 67, 70, 73, 76, 79, 82, 85, 88, 91, 94, 97])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(100)\n",
    "[b[a[::3]-i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 'Cat'), ('house', 'Dog'), ('dor', 'Mouse')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['car', 'house', 'dor']\n",
    "b = ['Cat', 'Dog', 'Mouse']\n",
    "list(zip(*[a,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking for ill-shaped lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 16070\n",
      "Number of rows with wrong length: 57:\n",
      "    fraction: 0.35 %\n"
     ]
    }
   ],
   "source": [
    "csvfile.seek(0);\n",
    "i = 0 #count total row number\n",
    "j = 0 #count false row number\n",
    "\n",
    "for row in reader:\n",
    "    i += 1\n",
    "    if(len(row) != 21):\n",
    "        #print('line size error in line {} \\n'.format(reader.line_num))\n",
    "        #print('line size is: {} \\n'.format(len(row)))\n",
    "        #print(*row)\n",
    "        #print('\\n')\n",
    "        j += 1\n",
    "\n",
    "print(\"\"\"Total number of rows: {0}\n",
    "Number of rows with wrong length: {1}:\n",
    "    fraction: {2:.2f} %\"\"\".format(i,j,(j/i)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['      ', '  ', '      ', '  ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\" {2,}\", \"Paul      Hager  er      hat  kein Geld.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lower' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d9db7e7a7284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dfsdfadSADSFDF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lower' is not defined"
     ]
    }
   ],
   "source": [
    "lower(\"dfsdfadSADSFDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = snowball.GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hallo',\n",
       " 'mein',\n",
       " 'Name',\n",
       " 'ist',\n",
       " 'Paul',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'jahreszahl',\n",
       " 'gebohren']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(filters)\n",
    "sentence = \"Hallo mein Name ist Paul ich bin 1992 gebohren\"\n",
    "filters.year_tracker(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jahreszahl'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"^[1][89][0-9]{2}$\",\"jahreszahl\", \"1993\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
