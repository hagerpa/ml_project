{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, re, pickle, itertools, progressbar, importlib\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from categories import categories\n",
    "import filters, vocabulary_tester, vocabulary_builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cats = categories()\n",
    "\n",
    "qcatfile = open('question_category_train.csv', 'r')\n",
    "qcatreader = csv.reader(qcatfile)\n",
    "\n",
    "next(qcatreader) # skipping column discription\n",
    "\n",
    "qcat_dict = {} # mapping from question_id to the parent category_id\n",
    "cat_freq = nltk.FreqDist() # maps the category_id to its\n",
    "\n",
    "for qcat in qcatreader:\n",
    "    cat_id = int(qcat[1])\n",
    "    pcat_id = cats.parent_id(cat_id)\n",
    "    q_id = int(qcat[2])\n",
    "    \n",
    "    qcat_dict[q_id] = pcat_id\n",
    "    cat_freq[cats.name(cat_id)] += 1\n",
    "    \n",
    "#cat_freq.plot()\n",
    "#cat_freq.most_common(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(filters)\n",
    "\n",
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "next(qreader)\n",
    "\n",
    "questions = []\n",
    "vocabulary = {}\n",
    "vocabulary['all'] = nltk.FreqDist()\n",
    "for cat_name in cats.all_names(): vocabulary[cat_name] = nltk.FreqDist()\n",
    "\n",
    "# Set this parameter to TRUE if you want to read through\n",
    "# all questions again, elsewise from file set to FALSE.\n",
    "NewRead = False \n",
    "\n",
    "total_enteties_count = 0\n",
    "\n",
    "if NewRead:\n",
    "    bar = progressbar.ProgressBar()\n",
    "    \n",
    "    for row in bar(qreader):\n",
    "        if len(row) != 21: continue\n",
    "        if row[15] != \"0\": continue\n",
    "            \n",
    "        if int(row[0]) in qcat_dict.keys():\n",
    "            cat_id = qcat_dict[int(row[0])]\n",
    "            \n",
    "            sentence = row[4].lower()\n",
    "            # running a sequence of filters on the raw question string \n",
    "            for filt in [filters.punctuation_filter]:\n",
    "                sentence = filt(sentence)\n",
    "            \n",
    "            words = word_tokenize(sentence)\n",
    "            # running a sequence of filtes on the already tokenized sentence\n",
    "            for filt in [filters.year_tracker, filters.small_word_filter, filters.stemming_filter]:\n",
    "                words = filt(words)\n",
    "            \n",
    "            questions.append( {\"words\":words, \"cat\":cats.name(cat_id)} )\n",
    "            vocabulary[ cats.name(cat_id) ] += nltk.FreqDist(words)\n",
    "            vocabulary['all'] += nltk.FreqDist(words)\n",
    "        \n",
    "    ## Saving into pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'wb'), open('vocabulary.pkl', 'wb')\n",
    "    pickle.dump(questions, q_file)\n",
    "    pickle.dump(vocabulary, v_file)\n",
    "    \n",
    "else:\n",
    "    ## Loading from pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'rb'), open('vocabulary.pkl', 'rb')\n",
    "    questions, vocabulary = pickle.load(q_file), pickle.load(v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(vocabulary_tester)\n",
    "importlib.reload(vocabulary_builders)\n",
    "from vocabulary_builders import most_common, most_common_reduced, ig_based, ig_based_non_uniform\n",
    "\n",
    "corpus = [(q['words'], q['category']) for q in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing IG-based Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 'ig_based (M = 350)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 50)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 150)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 450)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 100)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 300)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 250)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 200)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 400)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based (M = 500)' classifier...\n",
      " --- classifier is trained.\n",
      "\n",
      "testing 'ig_based (M = 300)' classifier: \n",
      " --- vocabulary volume:  2638\n",
      " --- standart accurcy: 0.61525\n",
      " --- uniform accuracy: 0.5166842108805063\n",
      "\n",
      "testing 'ig_based (M = 350)' classifier: \n",
      " --- vocabulary volume:  3065\n",
      " --- standart accurcy: 0.61525\n",
      " --- uniform accuracy: 0.509372620054371\n",
      "\n",
      "testing 'ig_based (M = 150)' classifier: \n",
      " --- vocabulary volume:  1217\n",
      " --- standart accurcy: 0.6025\n",
      " --- uniform accuracy: 0.5268852333123504\n",
      "\n",
      "testing 'ig_based (M = 450)' classifier: \n",
      " --- vocabulary volume:  3975\n",
      " --- standart accurcy: 0.6115\n",
      " --- uniform accuracy: 0.4939606808330133\n",
      "\n",
      "testing 'ig_based (M = 100)' classifier: \n",
      " --- vocabulary volume:  794\n",
      " --- standart accurcy: 0.59025\n",
      " --- uniform accuracy: 0.5237426028665977\n",
      "\n",
      "testing 'ig_based (M = 50)' classifier: \n",
      " --- vocabulary volume:  406\n",
      " --- standart accurcy: 0.562\n",
      " --- uniform accuracy: 0.5098949673777866\n",
      "\n",
      "testing 'ig_based (M = 250)' classifier: \n",
      " --- vocabulary volume:  2121\n",
      " --- standart accurcy: 0.6105\n",
      " --- uniform accuracy: 0.5229821698659611\n",
      "\n",
      "testing 'ig_based (M = 200)' classifier: \n",
      " --- vocabulary volume:  1673\n",
      " --- standart accurcy: 0.60725\n",
      " --- uniform accuracy: 0.5264570124404143\n",
      "\n",
      "testing 'ig_based (M = 400)' classifier: \n",
      " --- vocabulary volume:  3554\n",
      " --- standart accurcy: 0.61575\n",
      " --- uniform accuracy: 0.5025955956455562\n",
      "\n",
      "testing 'ig_based (M = 500)' classifier: \n",
      " --- vocabulary volume:  4385\n",
      " --- standart accurcy: 0.611\n",
      " --- uniform accuracy: 0.48698563903504055\n",
      "\n",
      "\n",
      " test are all finshed and saved into file!\n"
     ]
    }
   ],
   "source": [
    "vocab_builders = {}\n",
    "for M in np.arange(50,501,50):\n",
    "    name = \"ig_based (M = {0})\".format(M)\n",
    "    vocab_builders[ name ] = (ig_based, {\"frequencies\": vocabulary,\n",
    "                                         \"cat_frequencies\": cat_freq,\n",
    "                                         \"categories\": cats.all_names(),\n",
    "                                         \"M\": M,\n",
    "                                         \"read_from_file\": True})\n",
    "\n",
    "vocabularies = {}\n",
    "for vb_name in vocab_builders.keys():\n",
    "    vb, args = vocab_builders[vb_name]\n",
    "    vocabularies[vb_name] = vb(**args)\n",
    "\n",
    "vocabulary_tester.test(vocabularies, corpus, tr_set_size=10000, te_set_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 'ig_based_non_uniform (M = 1000)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based_non_uniform (M = 4000)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based_non_uniform (M = 7000)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based_non_uniform (M = 3000)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based_non_uniform (M = 2000)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based_non_uniform (M = 6000)' classifier...\n",
      " --- classifier is trained.\n",
      "training 'ig_based_non_uniform (M = 5000)' classifier...\n",
      " --- classifier is trained.\n",
      "\n",
      "testing 'ig_based_non_uniform (M = 1000)' classifier: \n",
      " --- vocabulary volume:  513\n",
      " --- standart accurcy: 0.57625\n",
      " --- uniform accuracy: 0.4994154476258803\n",
      "\n",
      "testing 'ig_based_non_uniform (M = 4000)' classifier: \n",
      " --- vocabulary volume:  2256\n",
      " --- standart accurcy: 0.6115\n",
      " --- uniform accuracy: 0.5119393048825388\n",
      "\n",
      "testing 'ig_based_non_uniform (M = 7000)' classifier: \n",
      " --- vocabulary volume:  3762\n",
      " --- standart accurcy: 0.616\n",
      " --- uniform accuracy: 0.507241637414363\n",
      "\n",
      "testing 'ig_based_non_uniform (M = 3000)' classifier: \n",
      " --- vocabulary volume:  1620\n",
      " --- standart accurcy: 0.603\n",
      " --- uniform accuracy: 0.5127820250986423\n",
      "\n",
      "testing 'ig_based_non_uniform (M = 2000)' classifier: \n",
      " --- vocabulary volume:  1068\n",
      " --- standart accurcy: 0.6015\n",
      " --- uniform accuracy: 0.5224145147366559\n",
      "\n",
      "testing 'ig_based_non_uniform (M = 6000)' classifier: \n",
      " --- vocabulary volume:  3251\n"
     ]
    }
   ],
   "source": [
    "vocab_builders = {}\n",
    "for M in np.arange(1000,7001,1000):\n",
    "    name = \"ig_based_non_uniform (M = {0})\".format(M)\n",
    "    vocab_builders[ name ] = (ig_based_non_uniform, {\"frequencies\": vocabulary,\n",
    "                                         \"cat_frequencies\": cat_freq,\n",
    "                                         \"categories\": cats.all_names(),\n",
    "                                         \"M\": M,\n",
    "                                         \"read_from_file\": True})\n",
    "\n",
    "vocabularies = {}\n",
    "for vb_name in vocab_builders.keys():\n",
    "    vb, args = vocab_builders[vb_name]\n",
    "    vocabularies[vb_name] = vb(**args)\n",
    "\n",
    "vocabulary_tester.test(vocabularies, corpus, tr_set_size=10000, te_set_size=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ##############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_reduced (M = 50, S = 12, MS = 50) done with M = 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 500) done with M = 500\n",
      "most_common_reduced (M = 400, S = 10, MS = 100) done with M = 400\n",
      "most_common_reduced (M = 100, S = 8, MS = 150) done with M = 100\n",
      "most_common_reduced (M = 350, S = 9, MS = 50) done with M = 350\n",
      "most_common_reduced (M = 450, S = 9, MS = 50) done with M = 450\n",
      "most_common_reduced (M = 50, S = 11, MS = 100) done with M = 50\n",
      "most_common_reduced (M = 500, S = 12, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 450, S = 11, MS = 50) done with M = 450\n",
      "most_common_reduced (M = 50, S = 9, MS = 150) done with M = 50\n",
      "most_common_reduced (M = 500, S = 8, MS = 50) done with M = 500\n",
      "most_common_reduced (M = 100, S = 9, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 350, S = 8, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 450, S = 12, MS = 150) done with M = 450\n",
      "most_common_reduced (M = 150, S = 12, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 450, S = 11, MS = 150) done with M = 450\n",
      "most_common_reduced (M = 500, S = 11, MS = 50) done with M = 500\n",
      "most_common_reduced (M = 350, S = 11, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 200, S = 8, MS = 100) done with M = 200\n",
      "most_common (M = 500) done with M = 500\n",
      "most_common_reduced (M = 100, S = 9, MS = 50) done with M = 100\n",
      "most_common_reduced (M = 150, S = 8, MS = 100) done with M = 150\n",
      "most_common_reduced (M = 450, S = 11, MS = 100) done with M = 450\n",
      "most_common_reduced (M = 300, S = 8, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 100, S = 10, MS = 150) done with M = 100\n",
      "most_common (M = 50) done with M = 50\n",
      "most_common_reduced (M = 500, S = 10, MS = 150) done with M = 500\n",
      "most_common_reduced (M = 300, S = 10, MS = 50) done with M = 300\n",
      "most_common_reduced (M = 300, S = 10, MS = 150) done with M = 300\n",
      "most_common_reduced (M = 150, S = 9, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 350, S = 10, MS = 50) done with M = 350\n",
      "most_common_reduced (M = 200, S = 12, MS = 50) done with M = 200\n",
      "most_common_reduced (M = 100, S = 9, MS = 150) done with M = 100\n",
      "most_common_reduced (M = 300, S = 10, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 150, S = 10, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 200, S = 10, MS = 100) done with M = 200\n",
      "most_common_reduced (M = 450, S = 9, MS = 100) done with M = 450\n",
      "most_common_reduced (M = 250, S = 9, MS = 50) done with M = 250\n",
      "most_common (M = 150) done with M = 150\n",
      "most_common_reduced (M = 250, S = 10, MS = 100) done with M = 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 200) done with M = 200\n",
      "most_common_reduced (M = 350, S = 8, MS = 150) done with M = 350\n",
      "most_common_reduced (M = 350, S = 9, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 400, S = 12, MS = 100) done with M = 400\n",
      "most_common_reduced (M = 100, S = 11, MS = 50) done with M = 100\n",
      "most_common_reduced (M = 500, S = 9, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 50, S = 8, MS = 50) done with M = 50\n",
      "most_common_reduced (M = 50, S = 11, MS = 150) done with M = 50\n",
      "most_common_reduced (M = 250, S = 12, MS = 100) done with M = 250\n",
      "most_common_reduced (M = 350, S = 11, MS = 150) done with M = 350\n",
      "most_common_reduced (M = 500, S = 11, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 150, S = 10, MS = 100) done with M = 150\n",
      "most_common (M = 350) done with M = 350\n",
      "most_common_reduced (M = 500, S = 10, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 500, S = 8, MS = 100) done with M = 500\n",
      "most_common_reduced (M = 200, S = 10, MS = 150) done with M = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 300) done with M = 300\n",
      "most_common (M = 200) done with M = 200\n",
      "most_common_reduced (M = 300, S = 9, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 400, S = 9, MS = 150) done with M = 400\n",
      "most_common_reduced (M = 200, S = 12, MS = 100) done with M = 200\n",
      "most_common_reduced (M = 50, S = 12, MS = 150) done with M = 50\n",
      "most_common_reduced (M = 350, S = 12, MS = 50) done with M = 350\n",
      "most_common_reduced (M = 100, S = 11, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 250, S = 12, MS = 150) done with M = 250\n",
      "most_common_reduced (M = 50, S = 9, MS = 50) done with M = 50\n",
      "most_common_reduced (M = 300, S = 11, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 200, S = 11, MS = 100) done with M = 200\n",
      "most_common_reduced (M = 200, S = 11, MS = 150) done with M = 200\n",
      "most_common_reduced (M = 450, S = 8, MS = 150) done with M = 450\n",
      "most_common_reduced (M = 150, S = 8, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 250, S = 11, MS = 100) done with M = 250\n",
      "most_common_reduced (M = 300, S = 12, MS = 150) done with M = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 100) done with M = 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 400) done with M = 400\n",
      "most_common_reduced (M = 350, S = 11, MS = 50) done with M = 350\n",
      "most_common_reduced (M = 100, S = 8, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 150, S = 9, MS = 150) done with M = 150\n",
      "most_common_reduced (M = 300, S = 12, MS = 100) done with M = 300\n",
      "most_common_reduced (M = 50, S = 12, MS = 100) done with M = 50\n",
      "most_common_reduced (M = 100, S = 11, MS = 150) done with M = 100\n",
      "most_common_reduced (M = 500, S = 9, MS = 150) done with M = 500\n",
      "most_common_reduced (M = 150, S = 11, MS = 50) done with M = 150\n",
      "most_common_reduced (M = 250, S = 8, MS = 150) done with M = 250\n",
      "most_common_reduced (M = 350, S = 12, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 350, S = 10, MS = 100) done with M = 350\n",
      "most_common_reduced (M = 150, S = 9, MS = 100) done with M = 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% ( 0 of 14) |                        | Elapsed Time: 0:00:00 ETA:  --:--:--"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_reduced (M = 400, S = 8, MS = 100) done with M = 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 150) done with M = 150\n",
      "most_common_reduced (M = 150, S = 11, MS = 100) done with M = 150\n",
      "most_common_reduced (M = 50, S = 8, MS = 150) done with M = 50\n",
      "most_common_reduced (M = 50, S = 9, MS = 100) done with M = 50\n",
      "most_common_reduced (M = 200, S = 9, MS = 100) done with M = 200\n",
      "most_common_reduced (M = 300, S = 8, MS = 50) done with M = 300\n",
      "most_common_reduced (M = 50, S = 8, MS = 100) done with M = 50\n",
      "most_common_reduced (M = 300, S = 9, MS = 150) done with M = 300\n",
      "most_common_reduced (M = 300, S = 8, MS = 150) done with M = 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 350) done with M = 350\n",
      "most_common_reduced (M = 100, S = 10, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 400, S = 10, MS = 50) done with M = 400\n",
      "most_common_reduced (M = 250, S = 10, MS = 150) done with M = 250\n",
      "most_common (M = 400) done with M = 400\n",
      "most_common_reduced (M = 250, S = 11, MS = 50) done with M = 250\n",
      "most_common_reduced (M = 200, S = 8, MS = 50) done with M = 200\n",
      "most_common_reduced (M = 250, S = 9, MS = 150) done with M = 250\n",
      "most_common (M = 100) done with M = 100\n",
      "most_common_reduced (M = 250, S = 12, MS = 50) done with M = 250\n",
      "most_common_reduced (M = 150, S = 8, MS = 150) done with M = 150\n",
      "most_common_reduced (M = 350, S = 9, MS = 150) done with M = 350\n",
      "most_common_reduced (M = 100, S = 12, MS = 50) done with M = 100\n",
      "most_common_reduced (M = 400, S = 9, MS = 50) done with M = 400\n",
      "most_common_reduced (M = 450, S = 9, MS = 150) done with M = 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig_based (M = 250) done with M = 250\n",
      "most_common_reduced (M = 250, S = 8, MS = 100) done with M = 250\n",
      "most_common_reduced (M = 400, S = 11, MS = 100) done with M = 400\n",
      "most_common_reduced (M = 500, S = 9, MS = 50) done with M = 500\n",
      "most_common_reduced (M = 100, S = 12, MS = 100) done with M = 100\n",
      "most_common_reduced (M = 100, S = 10, MS = 50) done with M = 100\n"
     ]
    }
   ],
   "source": [
    "for M in np.arange(50,501,50):\n",
    "    name = \"ig_based (M = {0})\".format(M)\n",
    "    vocab_builders[ name ] = (ig_based, {\"frequencies\": vocabulary,\n",
    "                                         \"cat_frequencies\": cat_freq,\n",
    "                                         \"categories\": cats.all_names(),\n",
    "                                         \"M\": M,\n",
    "                                         \"read_from_file\": True})\n",
    "    \n",
    "    name = \"most_common (M = {0})\".format(M)\n",
    "    vocab_builders[ name ] = (most_common, {\"frequencies\":vocabulary,\n",
    "                                            \"categories\": cats.all_names(),\n",
    "                                            \"M\": M})\n",
    "    \n",
    "    for S in np.arange(8,13):\n",
    "        for MS in [50,100,150]:\n",
    "            name = \"most_common_reduced (M = {0}, S = {1}, MS = {2})\".format(M,S,MS)\n",
    "            vocab_builders[ name ] = (most_common_reduced, {\"frequencies\":vocabulary,\n",
    "                                                                           \"categories\": cats.all_names(),\n",
    "                                                                           \"M\": M})\n",
    "\n",
    "vocabularies = {}\n",
    "for vb_name in vocab_builders.keys():\n",
    "    vb, args = vocab_builders[vb_name]\n",
    "    vocabularies[vb_name] = vb(**args)\n",
    "    print(vb_name,\"done with M =\",args[\"M\"])\n",
    "\n",
    "corpus = [(q['words'], q['category']) for q in questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING DIFFRENT FILTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = time.strftime(\"vacabulary_test_results/stemming_test_%d-%m-%Y_%H-%M.csv\", time.gmtime())\n",
    "res_file = open(file_name, 'w+')\n",
    "res_writer = csv.writer(res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(filters)\n",
    "\n",
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "next(qreader)\n",
    "\n",
    "questions = []\n",
    "vocabulary = {}\n",
    "vocabulary['all'] = nltk.FreqDist()\n",
    "for cat_name in cats.all_names(): vocabulary[cat_name] = nltk.FreqDist()\n",
    "\n",
    "# Set this parameter to TRUE if you want to read through\n",
    "# all questions again, elsewise from file set to FALSE.\n",
    "NewRead = False \n",
    "\n",
    "total_enteties_count = 0\n",
    "\n",
    "if NewRead:\n",
    "    bar = progressbar.ProgressBar()\n",
    "    \n",
    "    for row in bar(qreader):\n",
    "        if len(row) != 21: continue\n",
    "        if row[15] != \"0\": continue\n",
    "            \n",
    "        if int(row[0]) in qcat_dict.keys():\n",
    "            cat_id = qcat_dict[int(row[0])]\n",
    "            \n",
    "            sentence = row[4].lower()\n",
    "            # running a sequence of filters on the raw question string \n",
    "            for filt in [filters.punctuation_filter]:\n",
    "                sentence = filt(sentence)\n",
    "            \n",
    "            words = word_tokenize(sentence)\n",
    "            # running a sequence of filtes on the already tokenized sentence\n",
    "            for filt in [filters.small_word_filter, filters.stopword_filter]# filters.stemming_filter]:\n",
    "                words = filt(words)\n",
    "            \n",
    "            questions.append( {\"words\":words, \"cat\":cats.name(cat_id)} )\n",
    "            vocabulary[ cats.name(cat_id) ] += nltk.FreqDist(words)\n",
    "            vocabulary['all'] += nltk.FreqDist(words)\n",
    "        \n",
    "    ## Saving into pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'wb'), open('vocabulary.pkl', 'wb')\n",
    "    pickle.dump(questions, q_file)\n",
    "    pickle.dump(vocabulary, v_file)\n",
    "    \n",
    "else:\n",
    "    ## Loading from pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'rb'), open('vocabulary.pkl', 'rb')\n",
    "    questions, vocabulary = pickle.load(q_file), pickle.load(v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_builders[ \"most_common (M=300, no_stimming)\" ] = (most_common, {\"frequencies\":vocabulary,\n",
    "                                            \"categories\": cats.all_names(),\n",
    "                                            \"M\": 300})\n",
    "vocabulary_tester.test(vocabularies, corpus, tr_set_size=10000, te_set_size=4000, csv_file_writer = res_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(filters)\n",
    "\n",
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "next(qreader)\n",
    "\n",
    "questions = []\n",
    "vocabulary = {}\n",
    "vocabulary['all'] = nltk.FreqDist()\n",
    "for cat_name in cats.all_names(): vocabulary[cat_name] = nltk.FreqDist()\n",
    "\n",
    "# Set this parameter to TRUE if you want to read through\n",
    "# all questions again, elsewise from file set to FALSE.\n",
    "NewRead = False \n",
    "\n",
    "total_enteties_count = 0\n",
    "\n",
    "if NewRead:\n",
    "    bar = progressbar.ProgressBar()\n",
    "    \n",
    "    for row in bar(qreader):\n",
    "        if len(row) != 21: continue\n",
    "        if row[15] != \"0\": continue\n",
    "            \n",
    "        if int(row[0]) in qcat_dict.keys():\n",
    "            cat_id = qcat_dict[int(row[0])]\n",
    "            \n",
    "            sentence = row[4].lower()\n",
    "            # running a sequence of filters on the raw question string \n",
    "            for filt in [filters.punctuation_filter]:\n",
    "                sentence = filt(sentence)\n",
    "            \n",
    "            words = word_tokenize(sentence)\n",
    "            # running a sequence of filtes on the already tokenized sentence\n",
    "            for filt in [filters.small_word_filter, filters.stopword_filter, filters.stemming_filter]:\n",
    "                words = filt(words)\n",
    "            \n",
    "            questions.append( {\"words\":words, \"cat\":cats.name(cat_id)} )\n",
    "            vocabulary[ cats.name(cat_id) ] += nltk.FreqDist(words)\n",
    "            vocabulary['all'] += nltk.FreqDist(words)\n",
    "        \n",
    "    ## Saving into pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'wb'), open('vocabulary.pkl', 'wb')\n",
    "    pickle.dump(questions, q_file)\n",
    "    pickle.dump(vocabulary, v_file)\n",
    "    \n",
    "else:\n",
    "    ## Loading from pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'rb'), open('vocabulary.pkl', 'rb')\n",
    "    questions, vocabulary = pickle.load(q_file), pickle.load(v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_builders[ \"most_common (M=300, with_stimming)\" ] = (most_common, {\"frequencies\":vocabulary,\n",
    "                                            \"categories\": cats.all_names(),\n",
    "                                            \"M\": 300})\n",
    "vocabulary_tester.test(vocabularies, corpus, tr_set_size=10000, te_set_size=4000, csv_file_writer = res_writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
