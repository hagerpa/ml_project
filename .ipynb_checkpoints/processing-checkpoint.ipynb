{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "import pickle\n",
    "from categories import categories\n",
    "import filters\n",
    "import importlib\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: question_id               1: user_id                   2: sms_guru_id               3: category_main_id         \n",
      " 4: question                  5: description               6: tags                      7: categories               \n",
      " 8: url                       9: rating_count_positive    10: rating_count_negative    11: answer_count             \n",
      "12: reported                 13: answered                 14: active                   15: deleted                  \n",
      "16: seo_locked               17: editor_locked            18: editor_id                19: created_at               \n",
      "20: updated_at               "
     ]
    }
   ],
   "source": [
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "#cols = list(zip(np.arange(21),next(reader)))\n",
    "j = 0\n",
    "for i, q in zip(np.arange(21), next(qreader)):\n",
    "    if j == 3: l = \"\\n\"; j = 0;\n",
    "    else: l = \"\"; j += 1\n",
    "        \n",
    "    print('{0:2}: {1:25}'.format(i,q), end=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading out questions and tokenizing, checking vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = categories()\n",
    "\n",
    "qcatfile = open('question_category_train.csv', 'r')\n",
    "qcatreader = csv.reader(qcatfile)\n",
    "\n",
    "next(qcatreader) # skipping column discription\n",
    "\n",
    "qcat_dict = {} # mapping from question_id to the parent category_id\n",
    "catq_count = nltk.FreqDist() # maps the category_id to its\n",
    "\n",
    "for qcat in qcatreader:\n",
    "    cat_id = int(qcat[1])\n",
    "    pcat_id = cats.parent_id(cat_id)\n",
    "    q_id = int(qcat[2])\n",
    "    \n",
    "    qcat_dict[q_id] = pcat_id\n",
    "    catq_count[cats.name(cat_id)] += 1\n",
    "    \n",
    "#catq_count.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(filters)\n",
    "\n",
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "next(qreader)\n",
    "\n",
    "questions = []\n",
    "vocabulary = {}\n",
    "for cat_name in cats.all_names():\n",
    "    vocabulary[cat_name] = nltk.FreqDist()\n",
    "\n",
    "# Set this parameter to TRUE if you want to read through\n",
    "# all questions again, elsewise from file set to FALSE.\n",
    "NewRead = False \n",
    "\n",
    "if NewRead:\n",
    "    for row in qreader:\n",
    "        if len(row) == 21:\n",
    "            if int(row[0]) in qcat_dict.keys():\n",
    "                cat_id = qcat_dict[int(row[0])]\n",
    "                \n",
    "                sentence = row[4].lower()\n",
    "                # running a sequence of filters on the raw question string \n",
    "                for filt in [filters.punctuation_filter]:\n",
    "                    sentence = filt(sentence)\n",
    "                \n",
    "                words = word_tokenize(sentence)\n",
    "                # running a sequence of filtes on the already tokenized sentence\n",
    "                for filt in [filters.year_tracker, filters.small_word_filter, filters.stemming_filter]:\n",
    "                    words = filt(words)\n",
    "                \n",
    "                questions += [{\"words\": words, \"cat_id\": cat_id}]\n",
    "                vocabulary[ cats.name(cat_id) ] += nltk.FreqDist(words)\n",
    "        \n",
    "    ## Saving into pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'wb'), open('vocabulary.pkl', 'wb')\n",
    "    pickle.dump(questions, q_file)\n",
    "    pickle.dump(vocabulary, v_file)\n",
    "    \n",
    "else:\n",
    "    ## Loading from pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'rb'), open('vocabulary.pkl', 'rb')\n",
    "    questions, vocabulary = pickle.load(q_file), pickle.load(v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film_and_musik', 'stars_and_promis', 'computer_and_pc', 'alltag', 'namensforschung', 'literatur_and_sprache', 'schule', 'mensch_and_koerper', 'freizeit_and_sport', 'wissen', 'liebe_and_beziehung', 'astrologie', 'games_and_spiele', 'adult']\n",
      "[('wie', 47), ('ein', 47), ('ist', 45), ('ich', 44), ('man', 37), ('was', 36), ('die', 34), ('kann', 29), ('der', 24), ('auf', 22), ('es', 22), ('und', 22), ('wo', 19), ('bei', 19), ('in', 18), ('das', 18), ('welch', 16), ('den', 15), ('mit', 14), ('gibt', 14)]\n",
      "['ihr', 'kvm', 'croft', 'key', 'kommt', 'mein', 'auto', 'internetanbiet', 'gutscheinseit', 'sonst', 'geschwind', 'tb', 'hasst', 'ii', 'jed', 'werd', 'dauerhauft', 'hi', 'run', 'im']\n"
     ]
    }
   ],
   "source": [
    "print(cats.all_names())\n",
    "print(vocabulary['computer_and_pc'].most_common(20))\n",
    "print(list(vocabulary['computer_and_pc'])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_features(vocab, question):\n",
    "    features = {}\n",
    "    for v in vocab:\n",
    "        features[v] = v in question['words']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 150 # how much of the most common words one should take\n",
    "vocab = set()\n",
    "for cat_name in cats.all_names():\n",
    "    words = [w for w, f in vocabulary[cat_name].most_common(M)]\n",
    "    vocab = vocab.union(words)\n",
    "\n",
    "# words that appear in the most common words of more the #x=10 categories\n",
    "# are denoted as stoppwords.\n",
    "stopwords = set()\n",
    "for cat_names in itertools.combinations( cats.all_names(), 10):\n",
    "    cat_names = iter(cat_names)\n",
    "    sub_stops = set([w for w, f in vocabulary[next(cat_names)].most_common(M)])\n",
    "    for cat_name in cat_names:\n",
    "        sub_stops = sub_stops.intersection( set([w for w, f in vocabulary[cat_name].most_common(M)]) )\n",
    "    stopwords = stopwords.union(sub_stops)\n",
    "\n",
    "vocab = vocab.symmetric_difference(stopwords)\n",
    "\n",
    "feature_set = [(simple_features(vocab, q), cats.name(q['cat_id'])) for q in questions]\n",
    "train_set, test_set = feature_set[:10000], feature_set[10000:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\",\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " '06',\n",
       " '07',\n",
       " '08',\n",
       " '09',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '19',\n",
       " '20',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '26',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '30',\n",
       " 'ab',\n",
       " 'abend',\n",
       " 'aber',\n",
       " 'abkurz',\n",
       " 'abnehm',\n",
       " 'absend',\n",
       " 'abspritz',\n",
       " 'adress',\n",
       " 'aktivi',\n",
       " 'aktuell',\n",
       " 'album',\n",
       " 'alexandra',\n",
       " 'alina',\n",
       " 'also',\n",
       " 'anal',\n",
       " 'analsex',\n",
       " 'and',\n",
       " 'andreas',\n",
       " 'anfang',\n",
       " 'anmeld',\n",
       " 'anna',\n",
       " 'annika',\n",
       " 'antwort',\n",
       " 'arab',\n",
       " 'ashley',\n",
       " 'attack',\n",
       " 'auch',\n",
       " 'aug',\n",
       " 'auto',\n",
       " 'autogramm',\n",
       " 'autogrammadress',\n",
       " 'bahn',\n",
       " 'bahnhof',\n",
       " 'banan',\n",
       " 'band',\n",
       " 'bay',\n",
       " 'beantwort',\n",
       " 'beckham',\n",
       " 'bedeudet',\n",
       " 'bedeut',\n",
       " 'bedeuted',\n",
       " 'bedeutet',\n",
       " 'befried',\n",
       " 'begriff',\n",
       " 'beim',\n",
       " 'bekommt',\n",
       " 'beliebt',\n",
       " 'berg',\n",
       " 'berlin',\n",
       " 'besiegt',\n",
       " 'bess',\n",
       " 'best',\n",
       " 'besteh',\n",
       " 'besteht',\n",
       " 'beteutet',\n",
       " 'bezeichnet',\n",
       " 'bezieh',\n",
       " 'bianca',\n",
       " 'bieb',\n",
       " 'bill',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bitt',\n",
       " 'bizarr',\n",
       " 'blas',\n",
       " 'bleib',\n",
       " 'blue',\n",
       " 'blut',\n",
       " 'bohs',\n",
       " 'borussia',\n",
       " 'box',\n",
       " 'brauch',\n",
       " 'braucht',\n",
       " 'bravo',\n",
       " 'brem',\n",
       " 'bros',\n",
       " 'broth',\n",
       " 'brust',\n",
       " 'buch',\n",
       " 'buchstab',\n",
       " 'bundesliga',\n",
       " 'bushido',\n",
       " 'bzw',\n",
       " 'call',\n",
       " 'cd',\n",
       " 'champion',\n",
       " 'cheat',\n",
       " 'chines',\n",
       " 'christian',\n",
       " 'christiano',\n",
       " 'cinema',\n",
       " 'city',\n",
       " 'club',\n",
       " 'cm',\n",
       " 'cod',\n",
       " 'comput',\n",
       " 'cyrus',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'daniel',\n",
       " 'dank',\n",
       " 'dann',\n",
       " 'darf',\n",
       " 'dass',\n",
       " 'dauert',\n",
       " 'david',\n",
       " 'de',\n",
       " 'definiert',\n",
       " 'dein',\n",
       " 'denn',\n",
       " 'dennis',\n",
       " 'des',\n",
       " 'deutsch',\n",
       " 'deutschland',\n",
       " 'diablo',\n",
       " 'dich',\n",
       " 'dick',\n",
       " 'dies',\n",
       " 'dir',\n",
       " 'direkt',\n",
       " 'domin',\n",
       " 'dortmund',\n",
       " 'download',\n",
       " 'driv',\n",
       " 'ds',\n",
       " 'dsds',\n",
       " 'du',\n",
       " 'dud',\n",
       " 'durch',\n",
       " 'durchschnitt',\n",
       " 'duty',\n",
       " 'dvd',\n",
       " 'efron',\n",
       " 'ei',\n",
       " 'eier',\n",
       " 'eigenschaft',\n",
       " 'eigent',\n",
       " 'einwohn',\n",
       " 'elf',\n",
       " 'end',\n",
       " 'englisch',\n",
       " 'entstand',\n",
       " 'entsteh',\n",
       " 'entsteht',\n",
       " 'erd',\n",
       " 'erfund',\n",
       " 'erkennt',\n",
       " 'erst',\n",
       " 'ess',\n",
       " 'euro',\n",
       " 'ex',\n",
       " 'fahr',\n",
       " 'fahrt',\n",
       " 'fahrzeug',\n",
       " 'famili',\n",
       " 'familiennam',\n",
       " 'fan',\n",
       " 'fantasy',\n",
       " 'fc',\n",
       " 'fernseh',\n",
       " 'feucht',\n",
       " 'feuerwehr',\n",
       " 'fick',\n",
       " 'fifa',\n",
       " 'film',\n",
       " 'final',\n",
       " 'find',\n",
       " 'findet',\n",
       " 'fisch',\n",
       " 'flieg',\n",
       " 'folg',\n",
       " 'for',\n",
       " 'frag',\n",
       " 'frank',\n",
       " 'frankfurt',\n",
       " 'franziska',\n",
       " 'franzos',\n",
       " 'frau',\n",
       " 'frequenz',\n",
       " 'freund',\n",
       " 'freundin',\n",
       " 'fuhlt',\n",
       " 'fussball',\n",
       " 'fussballspiel',\n",
       " 'fussballtor',\n",
       " 'fussballverein',\n",
       " 'gab',\n",
       " 'gaga',\n",
       " 'gam',\n",
       " 'geb',\n",
       " 'gebor',\n",
       " 'geburstag',\n",
       " 'geburtstag',\n",
       " 'geg',\n",
       " 'gegrundet',\n",
       " 'geh',\n",
       " 'gehort',\n",
       " 'geht',\n",
       " 'geil',\n",
       " 'geld',\n",
       " 'genau',\n",
       " 'gerad',\n",
       " 'geschoss',\n",
       " 'geschrieb',\n",
       " 'geschwind',\n",
       " 'gespielt',\n",
       " 'gestorb',\n",
       " 'gesung',\n",
       " 'gewinn',\n",
       " 'gewinnt',\n",
       " 'gewonn',\n",
       " 'gezog',\n",
       " 'gi',\n",
       " 'glucklich',\n",
       " 'gold',\n",
       " 'gomez',\n",
       " 'googl',\n",
       " 'gott',\n",
       " 'grad',\n",
       " 'gran',\n",
       " 'gross',\n",
       " 'grosst',\n",
       " 'grupp',\n",
       " 'gta',\n",
       " 'guru',\n",
       " 'gut',\n",
       " 'haar',\n",
       " 'hallo',\n",
       " 'hamburg',\n",
       " 'handy',\n",
       " 'handynumm',\n",
       " 'harry',\n",
       " 'hatt',\n",
       " 'hauptmenu',\n",
       " 'hauptstadt',\n",
       " 'haus',\n",
       " 'haut',\n",
       " 'heik',\n",
       " 'heirat',\n",
       " 'heiss',\n",
       " 'heist',\n",
       " 'her',\n",
       " 'heut',\n",
       " 'hiess',\n",
       " 'hilton',\n",
       " 'hitl',\n",
       " 'hoch',\n",
       " 'hom',\n",
       " 'hotel',\n",
       " 'hubschraub',\n",
       " 'huhn',\n",
       " 'hund',\n",
       " 'icq',\n",
       " 'ihm',\n",
       " 'ihn',\n",
       " 'imm',\n",
       " 'install',\n",
       " 'internet',\n",
       " 'internetseit',\n",
       " 'ja',\n",
       " 'jackson',\n",
       " 'jahr',\n",
       " 'jahrestag',\n",
       " 'jahreszahl',\n",
       " 'jan',\n",
       " 'janin',\n",
       " 'japan',\n",
       " 'jed',\n",
       " 'jemal',\n",
       " 'jemand',\n",
       " 'jennif',\n",
       " 'jessica',\n",
       " 'jetzt',\n",
       " 'jugend',\n",
       " 'julia',\n",
       " 'jung',\n",
       " 'jungfrau',\n",
       " 'justin',\n",
       " 'kai',\n",
       " 'kalori',\n",
       " 'kart',\n",
       " 'katz',\n",
       " 'kauf',\n",
       " 'kein',\n",
       " 'kenn',\n",
       " 'kevin',\n",
       " 'kg',\n",
       " 'kind',\n",
       " 'kino',\n",
       " 'kinos',\n",
       " 'klein',\n",
       " 'klick',\n",
       " 'koln',\n",
       " 'komm',\n",
       " 'kommend',\n",
       " 'konn',\n",
       " 'konsol',\n",
       " 'korbchengross',\n",
       " 'korp',\n",
       " 'kostenlos',\n",
       " 'kostet',\n",
       " 'krankheit',\n",
       " 'kreb',\n",
       " 'krieg',\n",
       " 'kriegt',\n",
       " 'kuss',\n",
       " 'lad',\n",
       " 'lady',\n",
       " 'land',\n",
       " 'landkreis',\n",
       " 'lang',\n",
       " 'larissa',\n",
       " 'lass',\n",
       " 'latein',\n",
       " 'lauft',\n",
       " 'laura',\n",
       " 'laut',\n",
       " 'lautet',\n",
       " 'lea',\n",
       " 'leb',\n",
       " 'lebt',\n",
       " 'lesb',\n",
       " 'letzt',\n",
       " 'level',\n",
       " 'lieb',\n",
       " 'liebt',\n",
       " 'lied',\n",
       " 'liegt',\n",
       " 'liga',\n",
       " 'linda',\n",
       " 'link',\n",
       " 'lisa',\n",
       " 'lit',\n",
       " 'lol',\n",
       " 'losch',\n",
       " 'lotozahl',\n",
       " 'lotto',\n",
       " 'lottozahl',\n",
       " 'low',\n",
       " 'lvl',\n",
       " 'mach',\n",
       " 'macht',\n",
       " 'madch',\n",
       " 'mag',\n",
       " 'maik',\n",
       " 'mail',\n",
       " 'mal',\n",
       " 'mandy',\n",
       " 'manga',\n",
       " 'manhunt',\n",
       " 'mann',\n",
       " 'mannlich',\n",
       " 'mannschaft',\n",
       " 'maoam',\n",
       " 'map',\n",
       " 'marcel',\n",
       " 'mari',\n",
       " 'marina',\n",
       " 'mario',\n",
       " 'markus',\n",
       " 'masturbi',\n",
       " 'matthias',\n",
       " 'meer',\n",
       " 'mehr',\n",
       " 'mehrzahl',\n",
       " 'meist',\n",
       " 'melani',\n",
       " 'mell',\n",
       " 'mensch',\n",
       " 'mert',\n",
       " 'mich',\n",
       " 'michael',\n",
       " 'microsoft',\n",
       " 'miley',\n",
       " 'miriam',\n",
       " 'mitglied',\n",
       " 'mittwoch',\n",
       " 'mmorpg',\n",
       " 'mobil',\n",
       " 'mod',\n",
       " 'mond',\n",
       " 'moon',\n",
       " 'morg',\n",
       " 'mr',\n",
       " 'mtv',\n",
       " 'munch',\n",
       " 'muschi',\n",
       " 'musik',\n",
       " 'muss',\n",
       " 'nachfolg',\n",
       " 'nachnam',\n",
       " 'nacht',\n",
       " 'nackt',\n",
       " 'nadin',\n",
       " 'nah',\n",
       " 'nahm',\n",
       " 'nam',\n",
       " 'namenstag',\n",
       " 'namm',\n",
       " 'need',\n",
       " 'nehm',\n",
       " 'nennt',\n",
       " 'net',\n",
       " 'neu',\n",
       " 'new',\n",
       " 'nico',\n",
       " 'nina',\n",
       " 'nintendo',\n",
       " 'normal',\n",
       " 'numm',\n",
       " 'nur',\n",
       " 'nurnberg',\n",
       " 'ob',\n",
       " 'of',\n",
       " 'oft',\n",
       " 'oh',\n",
       " 'ohn',\n",
       " 'onani',\n",
       " 'onlin',\n",
       " 'orgasmus',\n",
       " 'ort',\n",
       " 'osterreich',\n",
       " 'paris',\n",
       " 'pass',\n",
       " 'passiert',\n",
       " 'passt',\n",
       " 'passwort',\n",
       " 'pat',\n",
       " 'patrick',\n",
       " 'paul',\n",
       " 'pc',\n",
       " 'penis',\n",
       " 'pet',\n",
       " 'pi',\n",
       " 'pickel',\n",
       " 'planet',\n",
       " 'playstation',\n",
       " 'pokemon',\n",
       " 'porno',\n",
       " 'pornos',\n",
       " 'pott',\n",
       " 'pro',\n",
       " 'promi',\n",
       " 'promis',\n",
       " 'prozent',\n",
       " 'ps',\n",
       " 'ps2',\n",
       " 'ps3',\n",
       " 'psp',\n",
       " 'punkt',\n",
       " 'pups',\n",
       " 'ramona',\n",
       " 'rapp',\n",
       " 'rauch',\n",
       " 'raus',\n",
       " 'recht',\n",
       " 'redewend',\n",
       " 'reich',\n",
       " 'richtig',\n",
       " 'robert',\n",
       " 'rock',\n",
       " 'roman',\n",
       " 'ronaldo',\n",
       " 'ronny',\n",
       " 'rtl',\n",
       " 'rund',\n",
       " 'sabrina',\n",
       " 'sag',\n",
       " 'sagt',\n",
       " 'saison',\n",
       " 'samstag',\n",
       " 'san',\n",
       " 'sandra',\n",
       " 'sang',\n",
       " 'sarah',\n",
       " 'satansbrat',\n",
       " 'schalk',\n",
       " 'schauspiel',\n",
       " 'schauspielerin',\n",
       " 'scheid',\n",
       " 'schluss',\n",
       " 'schnell',\n",
       " 'schoss',\n",
       " 'schrieb',\n",
       " 'schuh',\n",
       " 'schul',\n",
       " 'schutz',\n",
       " 'schutzengel',\n",
       " 'schwang',\n",
       " 'schwanz',\n",
       " 'schwarz',\n",
       " 'schweinegripp',\n",
       " 'schwer',\n",
       " 'schwerst',\n",
       " 'schwul',\n",
       " 'sehr',\n",
       " 'seit',\n",
       " 'sekund',\n",
       " 'selb',\n",
       " 'selena',\n",
       " 'send',\n",
       " 'sendung',\n",
       " 'sephiroth',\n",
       " 'seri',\n",
       " 'sex',\n",
       " 'sg',\n",
       " 'sieg',\n",
       " 'sieht',\n",
       " 'sim',\n",
       " 'simon',\n",
       " 'simpson',\n",
       " 'singl',\n",
       " 'singt',\n",
       " 'sinn',\n",
       " 'skorpion',\n",
       " 'smash',\n",
       " 'sms',\n",
       " 'smsguru',\n",
       " 'softwar',\n",
       " 'soll',\n",
       " 'song',\n",
       " 'sonic',\n",
       " 'sonn',\n",
       " 'spanisch',\n",
       " 'speed',\n",
       " 'sperma',\n",
       " 'spiel',\n",
       " 'spielt',\n",
       " 'spongebob',\n",
       " 'sportart',\n",
       " 'sprach',\n",
       " 'sprichwort',\n",
       " 'spruch',\n",
       " 'stadion',\n",
       " 'stadt',\n",
       " 'staffel',\n",
       " 'stalk',\n",
       " 'stamm',\n",
       " 'stammt',\n",
       " 'star',\n",
       " 'starb',\n",
       " 'stark',\n",
       " 'steff',\n",
       " 'steh',\n",
       " 'steht',\n",
       " 'steif',\n",
       " 'steigt',\n",
       " 'steinbock',\n",
       " 'stell',\n",
       " 'stellung',\n",
       " 'sterb',\n",
       " 'stern',\n",
       " 'sternzeich',\n",
       " 'stev',\n",
       " 'stier',\n",
       " 'strass',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'suss',\n",
       " 'sv',\n",
       " 'sven',\n",
       " 'tag',\n",
       " 'tal',\n",
       " 'taylor',\n",
       " 'teil',\n",
       " 'tel',\n",
       " 'telefonnumm',\n",
       " 'test',\n",
       " 'testament',\n",
       " 'teu',\n",
       " 'the',\n",
       " 'thomas',\n",
       " 'tief',\n",
       " 'tier',\n",
       " 'tim',\n",
       " 'tisdal',\n",
       " 'tokio',\n",
       " 'tor',\n",
       " 'train',\n",
       " 'treff',\n",
       " 'tsv',\n",
       " 'tun',\n",
       " 'turismo',\n",
       " 'turkisch',\n",
       " 'tut',\n",
       " 'tv',\n",
       " 'twilight',\n",
       " 'uber',\n",
       " 'ubersetz',\n",
       " 'ubersetzt',\n",
       " 'uhr',\n",
       " 'unbeliebt',\n",
       " 'undertak',\n",
       " 'universum',\n",
       " 'unlimeted',\n",
       " 'uns',\n",
       " 'unt',\n",
       " 'unterschied',\n",
       " 'urin',\n",
       " 'url',\n",
       " 'ursprung',\n",
       " 'usa',\n",
       " 'vagina',\n",
       " 'vanessa',\n",
       " 'variant',\n",
       " 'verdient',\n",
       " 'verein',\n",
       " 'verheiratet',\n",
       " 'verliebt',\n",
       " 'version',\n",
       " 'versteht',\n",
       " 'vfb',\n",
       " 'vfl',\n",
       " 'vic',\n",
       " 'video',\n",
       " 'vista',\n",
       " 'viva',\n",
       " 'vom',\n",
       " 'vor',\n",
       " 'vornam',\n",
       " 'vorwahl',\n",
       " 'waag',\n",
       " 'wan',\n",
       " 'warcraft',\n",
       " 'warfar',\n",
       " 'warm',\n",
       " 'wass',\n",
       " 'wassermann',\n",
       " 'watch',\n",
       " 'websit',\n",
       " 'weh',\n",
       " 'weiblich',\n",
       " 'weiss',\n",
       " 'weit',\n",
       " 'weltkrieg',\n",
       " 'weltweit',\n",
       " 'wem',\n",
       " 'wen',\n",
       " 'werbung',\n",
       " 'wett',\n",
       " 'widd',\n",
       " 'wied',\n",
       " 'wieg',\n",
       " 'wiegt',\n",
       " 'wieso',\n",
       " 'wii',\n",
       " 'will',\n",
       " 'windows',\n",
       " 'wir',\n",
       " 'wirklich',\n",
       " 'wiss',\n",
       " 'witz',\n",
       " 'wm',\n",
       " 'woch',\n",
       " 'wofur',\n",
       " 'woh',\n",
       " 'wohn',\n",
       " 'wohnt',\n",
       " 'woran',\n",
       " 'world',\n",
       " 'wort',\n",
       " 'wow',\n",
       " 'wrestl',\n",
       " 'wrestling',\n",
       " 'wurd',\n",
       " 'wurfel',\n",
       " 'wwe',\n",
       " 'www',\n",
       " 'xbox360',\n",
       " 'youtub',\n",
       " 'yu',\n",
       " 'yvonn',\n",
       " 'zac',\n",
       " 'zahl',\n",
       " 'zeit',\n",
       " 'zuerst',\n",
       " 'zug',\n",
       " 'zukunft',\n",
       " 'zur',\n",
       " 'zuruck',\n",
       " 'zusamm',\n",
       " 'zuzumau',\n",
       " 'zwilling',\n",
       " 'zwisch'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), len(stopwords)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wissen'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = {}\n",
    "example['words'] = word_tokenize(\"Wie heißt der Sänger  von U2\".lower())\n",
    "example\n",
    "cats.name(questions[q_id]['cat_id']), classifier.classify(simple_features(vocab, questions[q_id]))\n",
    "classifier.classify(simple_features(vocab,example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5626980534178361"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = [classifier.classify(q) for q, _ in test_set]\n",
    "indeed = [c for _, c in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      |                                                              l                      |\n",
      "                      |                                                              i                      |\n",
      "                      |                                                l             t                      |\n",
      "                      |                                  f      m      i             e                      |\n",
      "                      |                                  r      e      e             r                      |\n",
      "                      |                    s             e      n      b             a                      |\n",
      "                      |             n      t             i      s      e             t                      |\n",
      "                      |             a      a             z      c      _      f      u                      |\n",
      "                      |             m      r             e      h      a      i      r                      |\n",
      "                      |             e      s             i      _      n      l      _                      |\n",
      "                      |             n      _             t      a      d      m      a                      |\n",
      "                      |             s      a             _      n      _      _      n                    a |\n",
      "                      |             f      n             a      d      b      a      d                    s |\n",
      "                      |             o      d             n      _      e      n      _                    t |\n",
      "                      |             r      _             d      k      z      d      s                    r |\n",
      "                      |      w      s      p      s      _      o      i      _      p             a      o |\n",
      "                      |      i      c      r      c      s      e      e      m      r      a      l      l |\n",
      "                      |      s      h      o      h      p      r      h      u      a      d      l      o |\n",
      "                      |      s      u      m      u      o      p      u      s      c      u      t      g |\n",
      "                      |      e      n      i      l      r      e      n      i      h      l      a      i |\n",
      "                      |      n      g      s      e      t      r      g      k      e      t      g      e |\n",
      "----------------------+-------------------------------------------------------------------------------------+\n",
      "               wissen | <10.7%>  0.3%   0.5%   1.5%   0.6%   1.4%   0.1%   0.2%   0.2%   0.2%   0.2%   0.1% |\n",
      "      namensforschung |   0.5% <14.1%>  0.0%   0.2%   0.0%   0.0%      .      .   0.1%   0.0%      .      . |\n",
      "     stars_and_promis |   2.5%   0.1%  <7.7%>  0.2%   0.5%   0.2%   0.2%   0.6%   0.1%   0.1%   0.1%   0.0% |\n",
      "               schule |   3.7%   0.1%   0.2%  <3.6%>  0.2%   0.5%      .   0.0%   0.2%   0.0%   0.1%      . |\n",
      "   freizeit_and_sport |   1.9%   0.1%   0.9%   0.4%  <4.4%>  0.1%      .   0.1%   0.0%      .   0.2%   0.0% |\n",
      "   mensch_and_koerper |   3.1%   0.1%   0.2%   0.4%   0.1%  <3.2%>  0.2%   0.0%   0.1%   0.5%   0.1%   0.0% |\n",
      "  liebe_and_beziehung |   0.7%   0.1%   0.3%   0.1%   0.0%   0.6%  <3.4%>  0.1%   0.1%   0.2%   0.0%   0.0% |\n",
      "       film_and_musik |   1.2%   0.0%   0.9%   0.1%   0.1%   0.1%   0.0%  <2.6%>  0.1%   0.0%   0.1%   0.0% |\n",
      "literatur_and_sprache |   2.6%   0.7%   0.1%   0.1%      .   0.0%   0.0%   0.1%  <1.5%>  0.1%      .      . |\n",
      "                adult |   1.4%   0.1%   0.2%   0.1%   0.0%   1.0%   0.1%      .   0.1%  <1.6%>     .   0.0% |\n",
      "               alltag |   1.8%   0.1%   0.2%   0.4%   0.5%   0.1%   0.0%   0.1%   0.0%   0.0%  <0.9%>  0.0% |\n",
      "           astrologie |   0.4%   0.1%   0.2%   0.1%   0.1%   0.2%   0.7%   0.0%   0.0%   0.0%   0.1%  <1.6%>|\n",
      "----------------------+-------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = nltk.ConfusionMatrix(indeed, res)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words is 126685. \n",
      "The volume of the vocabulary is 18236.\n",
      "That makes an percentiage of 0.14\n"
     ]
    }
   ],
   "source": [
    "totNum = vocabulary.N()\n",
    "vocNum = vocabulary.B()\n",
    "print(\"\"\"The total number of words is {0}. \n",
    "The volume of the vocabulary is {1}.\n",
    "That makes an percentiage of {2:.2}\"\"\".format(totNum,vocNum,vocNum/totNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allwordlist = [w for q in questions for w in q[1]]\n",
    "fd = nltk.FreqDist(allwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ? x 11576   bedeutet x 1465       kommt x 947           '' x 649         oder x 506   \n",
      "       der x 4693         das x 1386          wo x 879       welche x 642         mein x 481   \n",
      "       wie x 3999         hat x 1351         wer x 810         gibt x 593          bei x 478   \n",
      "       was x 3919         von x 1260        eine x 781          den x 587         viel x 455   \n",
      "       ist x 3470         ein x 1135         mit x 769           er x 574         wenn x 452   \n",
      "       die x 2366          es x 1112        kann x 749          für x 562         welt x 430   \n",
      "       und x 2255         man x 1075       woher x 728        heißt x 554          sie x 423   \n",
      "       ich x 2011           , x 1061         auf x 701         sind x 548        haben x 415   \n",
      "        in x 1717           . x 1004       warum x 687           im x 542        viele x 415   \n",
      "      name x 1652        wann x 997           am x 683           zu x 530          aus x 380   \n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "delta = 0\n",
    "rows = 5\n",
    "epc = int(n/rows)\n",
    "a = np.arange(n) + delta\n",
    "\n",
    "for j in range(epc):\n",
    "    row = [fd.most_common(n)[i] for i in a[j::epc]]\n",
    "    for v in row:\n",
    "        print((\"{0:>10} x {1:<6}\").format(*v),end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- anschlag            - anstecken           - antikatalytische    - antonin             - antwortetst         \n",
      "- anschliesend        - ansteckend          - antike              - antonio             - anubis              \n",
      "- anschließen         - anstellen           - antimaterie         - antony              - anunis              \n",
      "- anschlägt           - antarktis           - antipinoxe          - antreten            - anus                \n",
      "- anschrift           - anteil              - antisemetismus      - antrieb             - anwalt              \n",
      "- anschwillt          - antenne             - antisemitismus      - antrittsvorlesung   - anwendungen         \n",
      "- ansehen             - antennenverhältnis  - antje               - anträge             - anwendungsgebiete   \n",
      "- ansprechen          - anthony             - anton               - antwort             - anwesen             \n",
      "- anspruch            - antibabypille       - antonella           - antworten           - anwesend            \n",
      "- anstatt             - antidepressiva      - antonia             - antwortet           - anwort              \n"
     ]
    }
   ],
   "source": [
    "vocabArr = sorted(list(vocab))\n",
    "\n",
    "n = 50\n",
    "delta = 2000\n",
    "rows = 5\n",
    "epc = int(n/rows)\n",
    "a = np.arange(n) + delta\n",
    "\n",
    "for j in range(epc):\n",
    "    row = [vocabArr[i] for i in a[j::epc]]\n",
    "    print((\"- {:20}\"*len(row)).format(*row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Was']\n",
      "['WIEVIEL']\n",
      "['Was']\n",
      "[]\n",
      "[]\n",
      "['Wie']\n",
      "['Wie']\n",
      "['Wie']\n",
      "[]\n",
      "['Was']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for q in questions[:10]:\n",
    "    print([w for w in q[1] if re.search('^W',w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001 1     1    \n",
      "00002 4     8    \n",
      "00003 9     27   \n",
      "00004 16    64   \n",
      "00005 25    125  \n",
      "00006 36    216  \n",
      "00007 49    343  \n",
      "00008 64    512  \n",
      "00009 81    729  \n",
      "00010 100   1000 \n"
     ]
    }
   ],
   "source": [
    "for x in range(1,11):\n",
    "    print(repr(x).zfill(5), repr(x**2).ljust(5),repr(x**3).ljust(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(names.words('male.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48,\n",
       "        51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99]),\n",
       " array([99,  2,  5,  8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47,\n",
       "        50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98]),\n",
       " array([98,  1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46,\n",
       "        49, 52, 55, 58, 61, 64, 67, 70, 73, 76, 79, 82, 85, 88, 91, 94, 97])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(100)\n",
    "[b[a[::3]-i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 'Cat'), ('house', 'Dog'), ('dor', 'Mouse')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['car', 'house', 'dor']\n",
    "b = ['Cat', 'Dog', 'Mouse']\n",
    "list(zip(*[a,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking for ill-shaped lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 16070\n",
      "Number of rows with wrong length: 57:\n",
      "    fraction: 0.35 %\n"
     ]
    }
   ],
   "source": [
    "csvfile.seek(0);\n",
    "i = 0 #count total row number\n",
    "j = 0 #count false row number\n",
    "\n",
    "for row in reader:\n",
    "    i += 1\n",
    "    if(len(row) != 21):\n",
    "        #print('line size error in line {} \\n'.format(reader.line_num))\n",
    "        #print('line size is: {} \\n'.format(len(row)))\n",
    "        #print(*row)\n",
    "        #print('\\n')\n",
    "        j += 1\n",
    "\n",
    "print(\"\"\"Total number of rows: {0}\n",
    "Number of rows with wrong length: {1}:\n",
    "    fraction: {2:.2f} %\"\"\".format(i,j,(j/i)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['      ', '  ', '      ', '  ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\" {2,}\", \"Paul      Hager  er      hat  kein Geld.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lower' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d9db7e7a7284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dfsdfadSADSFDF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lower' is not defined"
     ]
    }
   ],
   "source": [
    "lower(\"dfsdfadSADSFDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = snowball.GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hallo',\n",
       " 'mein',\n",
       " 'Name',\n",
       " 'ist',\n",
       " 'Paul',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'jahreszahl',\n",
       " 'gebohren']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(filters)\n",
    "sentence = \"Hallo mein Name ist Paul ich bin 1992 gebohren\"\n",
    "filters.year_tracker(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jahreszahl'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"^[1][89][0-9]{2}$\",\"jahreszahl\", \"1993\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
