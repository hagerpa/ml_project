{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "import pickle\n",
    "from categories import categories\n",
    "import filters\n",
    "import importlib\n",
    "\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: question_id               1: user_id                   2: sms_guru_id               3: category_main_id         \n",
      " 4: question                  5: description               6: tags                      7: categories               \n",
      " 8: url                       9: rating_count_positive    10: rating_count_negative    11: answer_count             \n",
      "12: reported                 13: answered                 14: active                   15: deleted                  \n",
      "16: seo_locked               17: editor_locked            18: editor_id                19: created_at               \n",
      "20: updated_at               "
     ]
    }
   ],
   "source": [
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "#cols = list(zip(np.arange(21),next(reader)))\n",
    "j = 0\n",
    "for i, q in zip(np.arange(21), next(qreader)):\n",
    "    if j == 3: l = \"\\n\"; j = 0;\n",
    "    else: l = \"\"; j += 1\n",
    "        \n",
    "    print('{0:2}: {1:25}'.format(i,q), end=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading out questions and tokenizing, checking vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = categories()\n",
    "\n",
    "qcatfile = open('question_category_train.csv', 'r')\n",
    "qcatreader = csv.reader(qcatfile)\n",
    "\n",
    "next(qcatreader) # skipping column discription\n",
    "\n",
    "qcat_dict = {} # mapping from question_id to the parent category_id\n",
    "cat_freq = nltk.FreqDist() # maps the category_id to its\n",
    "\n",
    "for qcat in qcatreader:\n",
    "    cat_id = int(qcat[1])\n",
    "    pcat_id = cats.parent_id(cat_id)\n",
    "    q_id = int(qcat[2])\n",
    "    \n",
    "    qcat_dict[q_id] = pcat_id\n",
    "    cat_freq[cats.name(cat_id)] += 1\n",
    "    \n",
    "cat_freq.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importlib.reload(filters)\n",
    "\n",
    "qfile = open('question_train.csv', 'r')\n",
    "qreader = csv.reader(qfile)\n",
    "\n",
    "qfile.seek(0);\n",
    "next(qreader)\n",
    "\n",
    "questions = []\n",
    "vocabulary = {}\n",
    "vocabulary['all'] = nltk.FreqDist()\n",
    "for cat_name in cats.all_names(): vocabulary[cat_name] = nltk.FreqDist()\n",
    "\n",
    "# Set this parameter to TRUE if you want to read through\n",
    "# all questions again, elsewise from file set to FALSE.\n",
    "NewRead = False \n",
    "\n",
    "if NewRead:\n",
    "    for row in qreader:\n",
    "        if len(row) == 21:\n",
    "            if int(row[0]) in qcat_dict.keys():\n",
    "                cat_id = qcat_dict[int(row[0])]\n",
    "                \n",
    "                sentence = row[4].lower()\n",
    "                # running a sequence of filters on the raw question string \n",
    "                for filt in [filters.punctuation_filter]:\n",
    "                    sentence = filt(sentence)\n",
    "                \n",
    "                words = word_tokenize(sentence)\n",
    "                # running a sequence of filtes on the already tokenized sentence\n",
    "                for filt in [filters.year_tracker, filters.small_word_filter, filters.stemming_filter]:\n",
    "                    words = filt(words)\n",
    "                \n",
    "                questions += [{\"words\": words, \"cat_id\": cat_id}]\n",
    "                vocabulary[ cats.name(cat_id) ] += nltk.FreqDist(words)\n",
    "                vocabulary['all'] += nltk.FreqDist(words)\n",
    "        \n",
    "    ## Saving into pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'wb'), open('vocabulary.pkl', 'wb')\n",
    "    pickle.dump(questions, q_file)\n",
    "    pickle.dump(vocabulary, v_file)\n",
    "    \n",
    "else:\n",
    "    ## Loading from pickle files\n",
    "    q_file, v_file = open('questions.pkl', 'rb'), open('vocabulary.pkl', 'rb')\n",
    "    questions, vocabulary = pickle.load(q_file), pickle.load(v_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film_and_musik', 'stars_and_promis', 'computer_and_pc', 'alltag', 'namensforschung', 'literatur_and_sprache', 'schule', 'mensch_and_koerper', 'freizeit_and_sport', 'wissen', 'liebe_and_beziehung', 'astrologie', 'games_and_spiele', 'adult']\n"
     ]
    }
   ],
   "source": [
    "print(cats.all_names())\n",
    "#print(vocabulary['computer_and_pc'].most_common(20))\n",
    "#print(list(vocabulary['computer_and_pc'])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14640 of 14640) |###################| Elapsed Time: 0:05:18 Time: 0:05:18\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "information_gain = {}\n",
    "\n",
    "bar = progressbar.ProgressBar()\n",
    "for w in bar(vocabulary['all']):\n",
    "    for cat in cats.all_names():\n",
    "        information_gain[(w, cat)] = 0\n",
    "        if vocabulary[cat].freq(w) > 0:\n",
    "            information_gain[(w, cat)] = vocabulary[cat].freq(w) * cat_freq.freq(cat) \\\n",
    "                                        * np.log( vocabulary[cat].freq(w) / vocabulary['all'].freq(w) )\n",
    "        if vocabulary[cat].freq(w) < 1:\n",
    "            information_gain[(w, cat)] += (1 - vocabulary[cat].freq(w)) * cat_freq.freq(cat) \\\n",
    "                                        * np.log( (1 - vocabulary[cat].freq(w)) / (1 -vocabulary['all'].freq(w)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (14 of 14) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "794\n",
      "classifier done\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "# ten best words by information_gain\n",
    "vocab_ig = set()\n",
    "\n",
    "bar = progressbar.ProgressBar()\n",
    "for cat in bar(cats.all_names()):\n",
    "    lis = [(information_gain[(w,cat)], w) for w in vocabulary['all']]\n",
    "    lis.sort()\n",
    "    vocab_ig = vocab_ig.union([w for _, w in lis[-100:]])\n",
    "    \n",
    "print(len(vocab_ig))\n",
    "\n",
    "feature_set_ig = [(simple_features(vocab_ig, q), cats.name(q['cat_id'])) for q in questions]\n",
    "train_set_ig, test_set_ig = feature_set_ig[:10000], feature_set_ig[10000:]\n",
    "classifier_ig = nltk.NaiveBayesClassifier.train(train_set_ig)\n",
    "\n",
    "print('classifier done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5921231326392032"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier_ig, test_set_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_features(vocab, question):\n",
    "    features = {}\n",
    "    for v in vocab:\n",
    "        features[v] = v in question['words']\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = 150 # how much of the most common words one should take\n",
    "vocab = set()\n",
    "for cat_name in cats.all_names():\n",
    "    words = [w for w,_ in vocabulary[cat_name].most_common(M)]\n",
    "    vocab = vocab.union(words)\n",
    "\n",
    "# words that appear in the most common words of more the #x=10 categories\n",
    "# are denoted as stoppwords.\n",
    "stopwords = set()\n",
    "for cat_names in itertools.combinations( cats.all_names(), 10):\n",
    "    cat_names = iter(cat_names)\n",
    "    sub_stops = set([w for w, f in vocabulary[next(cat_names)].most_common(M)])\n",
    "    for cat_name in cat_names:\n",
    "        sub_stops = sub_stops.intersection( set([w for w, f in vocabulary[cat_name].most_common(M)]) )\n",
    "    stopwords = stopwords.union(sub_stops)\n",
    "\n",
    "#vocab = vocab.symmetric_difference(stopwords)\n",
    "\n",
    "training_set_size, test_set_size = 5000, 1000\n",
    "\n",
    "feature_set = [(simple_features(vocab, q), cats.name(q['cat_id'])) for q in questions]\n",
    "train_set, test_set = feature_set[:training_set_size], feature_set[test_set_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 63)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-f03854f9d79b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/classify/util.py\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(classifier, gold)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/classify/api.py\u001b[0m in \u001b[0;36mclassify_many\u001b[0;34m(self, featuresets)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/classify/api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/classify/naivebayes.py\u001b[0m in \u001b[0;36mprob_classify\u001b[0;34m(self, featureset)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                     \u001b[0mfeature_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feature_probdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mlogprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfeature_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0;31m# nb: This case will never come up if the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36mlogprob\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# Default definition, in terms of prob()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_NINF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/nltk/probability.py\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freqdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_freqdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_divisor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      |                                                              l                      |\n",
      "                      |                                                              i                      |\n",
      "                      |                                                l             t                      |\n",
      "                      |                                  f      m      i             e                      |\n",
      "                      |                                  r      e      e             r                      |\n",
      "                      |                    s             e      n      b             a                      |\n",
      "                      |             n      t             i      s      e             t                      |\n",
      "                      |             a      a             z      c      _      f      u                      |\n",
      "                      |             m      r             e      h      a      i      r                      |\n",
      "                      |             e      s             i      _      n      l      _                      |\n",
      "                      |             n      _             t      a      d      m      a                      |\n",
      "                      |             s      a             _      n      _      _      n                    a |\n",
      "                      |             f      n             a      d      b      a      d                    s |\n",
      "                      |             o      d             n      _      e      n      _                    t |\n",
      "                      |             r      _             d      k      z      d      s                    r |\n",
      "                      |      w      s      p      s      _      o      i      _      p             a      o |\n",
      "                      |      i      c      r      c      s      e      e      m      r      a      l      l |\n",
      "                      |      s      h      o      h      p      r      h      u      a      d      l      o |\n",
      "                      |      s      u      m      u      o      p      u      s      c      u      t      g |\n",
      "                      |      e      n      i      l      r      e      n      i      h      l      a      i |\n",
      "                      |      n      g      s      e      t      r      g      k      e      t      g      e |\n",
      "----------------------+-------------------------------------------------------------------------------------+\n",
      "               wissen | <10.7%>  0.3%   0.5%   1.5%   0.6%   1.4%   0.1%   0.2%   0.2%   0.2%   0.2%   0.1% |\n",
      "      namensforschung |   0.5% <14.1%>  0.0%   0.2%   0.0%   0.0%      .      .   0.1%   0.0%      .      . |\n",
      "     stars_and_promis |   2.5%   0.1%  <7.7%>  0.2%   0.5%   0.2%   0.2%   0.6%   0.1%   0.1%   0.1%   0.0% |\n",
      "               schule |   3.7%   0.1%   0.2%  <3.6%>  0.2%   0.5%      .   0.0%   0.2%   0.0%   0.1%      . |\n",
      "   freizeit_and_sport |   1.9%   0.1%   0.9%   0.4%  <4.4%>  0.1%      .   0.1%   0.0%      .   0.2%   0.0% |\n",
      "   mensch_and_koerper |   3.1%   0.1%   0.2%   0.4%   0.1%  <3.2%>  0.2%   0.0%   0.1%   0.5%   0.1%   0.0% |\n",
      "  liebe_and_beziehung |   0.7%   0.1%   0.3%   0.1%   0.0%   0.6%  <3.4%>  0.1%   0.1%   0.2%   0.0%   0.0% |\n",
      "       film_and_musik |   1.2%   0.0%   0.9%   0.1%   0.1%   0.1%   0.0%  <2.6%>  0.1%   0.0%   0.1%   0.0% |\n",
      "literatur_and_sprache |   2.6%   0.7%   0.1%   0.1%      .   0.0%   0.0%   0.1%  <1.5%>  0.1%      .      . |\n",
      "                adult |   1.4%   0.1%   0.2%   0.1%   0.0%   1.0%   0.1%      .   0.1%  <1.6%>     .   0.0% |\n",
      "               alltag |   1.8%   0.1%   0.2%   0.4%   0.5%   0.1%   0.0%   0.1%   0.0%   0.0%  <0.9%>  0.0% |\n",
      "           astrologie |   0.4%   0.1%   0.2%   0.1%   0.1%   0.2%   0.7%   0.0%   0.0%   0.0%   0.1%  <1.6%>|\n",
      "----------------------+-------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = [classifier.classify(q) for q, _ in test_set]\n",
    "indeed = [c for _, c in test_set]\n",
    "\n",
    "cm = nltk.ConfusionMatrix(indeed, res)\n",
    "print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words is 126685. \n",
      "The volume of the vocabulary is 18236.\n",
      "That makes an percentiage of 0.14\n"
     ]
    }
   ],
   "source": [
    "totNum = vocabulary.N()\n",
    "vocNum = vocabulary.B()\n",
    "print(\"\"\"The total number of words is {0}. \n",
    "The volume of the vocabulary is {1}.\n",
    "That makes an percentiage of {2:.2}\"\"\".format(totNum,vocNum,vocNum/totNum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allwordlist = [w for q in questions for w in q[1]]\n",
    "fd = nltk.FreqDist(allwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ? x 11576   bedeutet x 1465       kommt x 947           '' x 649         oder x 506   \n",
      "       der x 4693         das x 1386          wo x 879       welche x 642         mein x 481   \n",
      "       wie x 3999         hat x 1351         wer x 810         gibt x 593          bei x 478   \n",
      "       was x 3919         von x 1260        eine x 781          den x 587         viel x 455   \n",
      "       ist x 3470         ein x 1135         mit x 769           er x 574         wenn x 452   \n",
      "       die x 2366          es x 1112        kann x 749          für x 562         welt x 430   \n",
      "       und x 2255         man x 1075       woher x 728        heißt x 554          sie x 423   \n",
      "       ich x 2011           , x 1061         auf x 701         sind x 548        haben x 415   \n",
      "        in x 1717           . x 1004       warum x 687           im x 542        viele x 415   \n",
      "      name x 1652        wann x 997           am x 683           zu x 530          aus x 380   \n"
     ]
    }
   ],
   "source": [
    "n = 50\n",
    "delta = 0\n",
    "rows = 5\n",
    "epc = int(n/rows)\n",
    "a = np.arange(n) + delta\n",
    "\n",
    "for j in range(epc):\n",
    "    row = [fd.most_common(n)[i] for i in a[j::epc]]\n",
    "    for v in row:\n",
    "        print((\"{0:>10} x {1:<6}\").format(*v),end=\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- anschlag            - anstecken           - antikatalytische    - antonin             - antwortetst         \n",
      "- anschliesend        - ansteckend          - antike              - antonio             - anubis              \n",
      "- anschließen         - anstellen           - antimaterie         - antony              - anunis              \n",
      "- anschlägt           - antarktis           - antipinoxe          - antreten            - anus                \n",
      "- anschrift           - anteil              - antisemetismus      - antrieb             - anwalt              \n",
      "- anschwillt          - antenne             - antisemitismus      - antrittsvorlesung   - anwendungen         \n",
      "- ansehen             - antennenverhältnis  - antje               - anträge             - anwendungsgebiete   \n",
      "- ansprechen          - anthony             - anton               - antwort             - anwesen             \n",
      "- anspruch            - antibabypille       - antonella           - antworten           - anwesend            \n",
      "- anstatt             - antidepressiva      - antonia             - antwortet           - anwort              \n"
     ]
    }
   ],
   "source": [
    "vocabArr = sorted(list(vocab))\n",
    "\n",
    "n = 50\n",
    "delta = 2000\n",
    "rows = 5\n",
    "epc = int(n/rows)\n",
    "a = np.arange(n) + delta\n",
    "\n",
    "for j in range(epc):\n",
    "    row = [vocabArr[i] for i in a[j::epc]]\n",
    "    print((\"- {:20}\"*len(row)).format(*row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Was']\n",
      "['WIEVIEL']\n",
      "['Was']\n",
      "[]\n",
      "[]\n",
      "['Wie']\n",
      "['Wie']\n",
      "['Wie']\n",
      "[]\n",
      "['Was']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for q in questions[:10]:\n",
    "    print([w for w in q[1] if re.search('^W',w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00001 1     1    \n",
      "00002 4     8    \n",
      "00003 9     27   \n",
      "00004 16    64   \n",
      "00005 25    125  \n",
      "00006 36    216  \n",
      "00007 49    343  \n",
      "00008 64    512  \n",
      "00009 81    729  \n",
      "00010 100   1000 \n"
     ]
    }
   ],
   "source": [
    "for x in range(1,11):\n",
    "    print(repr(x).zfill(5), repr(x**2).ljust(5),repr(x**3).ljust(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(names.words('male.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48,\n",
       "        51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93, 96, 99]),\n",
       " array([99,  2,  5,  8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47,\n",
       "        50, 53, 56, 59, 62, 65, 68, 71, 74, 77, 80, 83, 86, 89, 92, 95, 98]),\n",
       " array([98,  1,  4,  7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46,\n",
       "        49, 52, 55, 58, 61, 64, 67, 70, 73, 76, 79, 82, 85, 88, 91, 94, 97])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(100)\n",
    "[b[a[::3]-i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 'Cat'), ('house', 'Dog'), ('dor', 'Mouse')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['car', 'house', 'dor']\n",
    "b = ['Cat', 'Dog', 'Mouse']\n",
    "list(zip(*[a,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# checking for ill-shaped lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows: 16070\n",
      "Number of rows with wrong length: 57:\n",
      "    fraction: 0.35 %\n"
     ]
    }
   ],
   "source": [
    "csvfile.seek(0);\n",
    "i = 0 #count total row number\n",
    "j = 0 #count false row number\n",
    "\n",
    "for row in reader:\n",
    "    i += 1\n",
    "    if(len(row) != 21):\n",
    "        #print('line size error in line {} \\n'.format(reader.line_num))\n",
    "        #print('line size is: {} \\n'.format(len(row)))\n",
    "        #print(*row)\n",
    "        #print('\\n')\n",
    "        j += 1\n",
    "\n",
    "print(\"\"\"Total number of rows: {0}\n",
    "Number of rows with wrong length: {1}:\n",
    "    fraction: {2:.2f} %\"\"\".format(i,j,(j/i)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['      ', '  ', '      ', '  ']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(\" {2,}\", \"Paul      Hager  er      hat  kein Geld.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lower' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-d9db7e7a7284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dfsdfadSADSFDF\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lower' is not defined"
     ]
    }
   ],
   "source": [
    "lower(\"dfsdfadSADSFDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = snowball.GermanStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hallo',\n",
       " 'mein',\n",
       " 'Name',\n",
       " 'ist',\n",
       " 'Paul',\n",
       " 'ich',\n",
       " 'bin',\n",
       " 'jahreszahl',\n",
       " 'gebohren']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(filters)\n",
    "sentence = \"Hallo mein Name ist Paul ich bin 1992 gebohren\"\n",
    "filters.year_tracker(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jahreszahl'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"^[1][89][0-9]{2}$\",\"jahreszahl\", \"1993\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
